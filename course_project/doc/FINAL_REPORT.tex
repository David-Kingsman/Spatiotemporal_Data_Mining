\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{float}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}

% Page setup
\geometry{margin=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue,
    pdftitle={Spatio-temporal Land Surface Temperature Interpolation},
    pdfauthor={Your Name}
}

% Title information
\title{Spatio-temporal Land Surface Temperature Interpolation: \\
A Probabilistic Deep Learning and Gaussian Process Approach}
\author{Your Name}
\date{November 2024}

\begin{document}

\maketitle

\begin{abstract}
Land Surface Temperature (LST) data from Moderate Resolution Imaging Spectroradiometer (MODIS) often contains significant missing values due to cloud cover, sensor limitations, and atmospheric conditions. This study addresses the LST interpolation problem by developing and comparing three probabilistic spatio-temporal models: a Probabilistic U-Net (deep learning), a Gradient Boosting Tree model (XGBoost), and a Sparse Variational Gaussian Process (SVGP) with three distinct kernel designs (separable, additive, and non-separable space-time kernels). Unlike previous work that treats time as a categorical variable, we explicitly model temporal correlations using various space-time kernel structures. All models provide probabilistic predictions with uncertainty quantification, evaluated using both regression metrics (RMSE, R\textsuperscript{2}) and probabilistic metrics (CRPS, prediction interval coverage). On a MODIS LST dataset spanning 31 days over a $100 \times 200$ spatial grid, the U-Net model achieved the best overall performance with RMSE = 1.14 K, R\textsuperscript{2} = 0.982, and CRPS = 0.76 K, significantly outperforming the tree-based and GP models. Our key methodological contributions include: (1) demonstrating that probabilistic deep learning (U-Net) significantly outperforms traditional approaches for LST interpolation, and (2) successfully comparing three spatio-temporal kernel designs in GP, finding that the separable kernel performs best on MODIS LST data, challenging the common assumption that non-separable kernels are always superior. Our work contributes a reusable Python library (\texttt{lstinterp}) with unified APIs for data loading, model training, and evaluation, facilitating reproducible research and application to other spatio-temporal interpolation tasks.

\vspace{0.2cm}
\noindent\textbf{Keywords:} Land Surface Temperature, Spatio-temporal Interpolation, Gaussian Process, Deep Learning, Uncertainty Quantification
\end{abstract}

\section{Introduction}

\subsection{Background and Motivation}

Land Surface Temperature (LST) is a critical variable in earth system science, influencing climate modeling, agriculture, water resource management, and urban heat island studies \citep{li2013satellite, wan2015modis}. Satellite-based LST measurements, particularly from MODIS instruments, provide valuable global coverage but suffer from systematic data gaps due to cloud cover, sensor failures, and atmospheric interference \citep{wan2014new}. Effective interpolation methods are essential to reconstruct complete spatio-temporal LST fields for downstream applications.

Traditional interpolation methods such as kriging, inverse distance weighting (IDW), and bilinear interpolation have been widely used but often fail to capture complex non-linear spatio-temporal dependencies. Furthermore, these methods typically provide only point estimates without uncertainty quantification, limiting their usefulness in decision-making contexts.

\subsection{Related Work}

Previous studies on LST interpolation have employed various approaches:

\begin{itemize}
\item \textbf{Classical Methods}: Kriging-based methods \citep{li2011review, hengl2007about} have shown moderate success but require strong assumptions about stationarity and variogram structure.
\item \textbf{Machine Learning}: Random Forest and Support Vector Regression \citep{li2011application, appelhans2015evaluating} have been applied to LST interpolation, but typically treat temporal information as categorical variables, losing temporal correlation.
\item \textbf{Gaussian Processes}: Recent work has used GP for spatial interpolation \citep{zhang2021gaussian, wang2017online}, but few studies have explicitly modeled spatio-temporal correlations using separable kernels.
\end{itemize}

Our work extends previous approaches by: (1) explicitly modeling spatio-temporal correlations using three distinct kernel designs (separable, additive, and non-separable) in GP, (2) providing probabilistic predictions with uncertainty quantification for all models, and (3) developing a reusable software framework for reproducible research.

\subsection{Objectives and Contributions}

The main objectives of this study are:

\begin{enumerate}
\item \textbf{Develop three probabilistic spatio-temporal models} for LST interpolation: Probabilistic U-Net, XGBoost with quantile regression, and Sparse Variational GP with multiple kernel designs (separable, additive, and non-separable).
\item \textbf{Provide comprehensive uncertainty quantification} using CRPS, prediction intervals, and calibration metrics.
\item \textbf{Compare model performance} on a real MODIS LST dataset using both regression and probabilistic metrics.
\item \textbf{Develop a reusable Python library} (\texttt{lstinterp}) with unified APIs for easy application to other spatio-temporal problems.
\end{enumerate}

\textbf{Key Contributions:}

\begin{itemize}
\item \textbf{Methodological}: (1) Demonstration that probabilistic deep learning (U-Net) significantly outperforms traditional approaches for LST interpolation. (2) Introduction and comparison of three distinct space-time kernel designs (separable, additive, and non-separable) in GP for explicit temporal correlation modeling, finding that the separable kernel performs best on MODIS LST data, challenging the common assumption that non-separable kernels are always superior.
\item \textbf{Technical}: Development of a probabilistic U-Net architecture adapted for LST image inpainting with pixel-level uncertainty estimation.
\item \textbf{Engineering}: Creation of a modular, reusable Python library (\texttt{lstinterp}) with consistent APIs for data loading, model training, and evaluation.
\end{itemize}

\section{Methodology}

\subsection{Problem Formulation}

We formulate the LST interpolation problem as follows:

Given a 3D tensor $\mathbf{T} \in \mathbb{R}^{H \times W \times T}$ representing LST observations over $H$ latitude bins, $W$ longitude bins, and $T$ time steps, where $T_{h,w,t} = 0$ indicates missing data, our goal is to learn a function $f: (\text{lat}, \text{lon}, t) \rightarrow y$ that predicts LST values at any spatio-temporal location, along with predictive uncertainty.

\subsection{Data Preprocessing}

\textbf{Training-Test Split}: We strictly use the provided \texttt{training\_tensor} for model training and cross-validation, and \texttt{test\_tensor} for final evaluation only.

\textbf{Normalization}:
\begin{itemize}
\item \textbf{Point-based models (GP, Tree)}: Input coordinates are min-max normalized to [0, 1], and target values are Z-score normalized.
\item \textbf{Image-based models (U-Net)}: Images are Z-score normalized using training set statistics ($\mu_{\text{train}}$, $\sigma_{\text{train}}$).
\end{itemize}

\textbf{Missing Value Handling}: Missing values are masked during training and prediction. For U-Net, missing pixels are set to 0 (later concatenated with a binary mask).

\subsection{Model Architectures}

\subsubsection{Probabilistic U-Net}

We adapt the U-Net architecture \citep{ronneberger2015u} for probabilistic image inpainting. The model takes as input a concatenated tensor of the LST image and a binary mask $\mathbf{M} \in \{0,1\}^{H \times W}$ (1 = observed, 0 = missing).

\textbf{Architecture}:
\begin{itemize}
\item \textbf{Encoder}: 2 convolutional blocks, each with 2$\times$Conv2d$\rightarrow$ReLU$\rightarrow$BatchNorm2d, followed by MaxPool2d
\item \textbf{Bottleneck}: 1 convolutional block with expanded channels
\item \textbf{Decoder}: 2 upsampling blocks using ConvTranspose2d, concatenation with encoder features, and 2$\times$Conv2d$\rightarrow$ReLU$\rightarrow$BatchNorm2d
\item \textbf{Output Heads}: Separate heads for mean $\mu$ and log-variance $\log \sigma^2$ (clamped to [-10, 10] for numerical stability)
\end{itemize}

\textbf{Loss Function}: Gaussian Negative Log-Likelihood (NLL), computed only on observed pixels:

\begin{equation}
\mathcal{L} = \frac{1}{|\mathcal{M}|} \sum_{(h,w) \in \mathcal{M}} \left[ \frac{1}{2}\log \sigma^2_{h,w} + \frac{(y_{h,w} - \mu_{h,w})^2}{2\sigma^2_{h,w}} \right]
\end{equation}

where $\mathcal{M}$ is the set of observed pixels.

\textbf{Hyperparameters}:
\begin{itemize}
\item Base channels: 32
\item Learning rate: $5 \times 10^{-4}$
\item Batch size: 4
\item Dropout: 0.2
\item Weight decay: $10^{-5}$
\item Gradient clipping: max\_norm = 1.0
\end{itemize}

\subsubsection{Tree-based Model (XGBoost with Quantile Regression)}

We use XGBoost \citep{chen2016xgboost} with quantile regression to provide probabilistic predictions. The model takes point-based features $\mathbf{x} = (\text{lat}, \text{lon}, t)$ and outputs quantiles.

\textbf{Training}: We train separate models for quantiles $q \in \{0.1, 0.5, 0.9\}$ using XGBoost's quantile regression objective.

\textbf{Prediction}: For each test point, we predict:
\begin{itemize}
\item Mean: $\mu = \hat{y}_{0.5}$ (median)
\item Standard deviation: $\sigma = (\hat{y}_{0.9} - \hat{y}_{0.1}) / (2 \times \Phi^{-1}(0.9))$ where $\Phi$ is the standard normal CDF
\end{itemize}

\textbf{Hyperparameters}:
\begin{itemize}
\item Number of estimators: 100
\item Max depth: 6
\item Objective: \texttt{reg:quantileerror}
\end{itemize}

\subsubsection{Sparse Variational Gaussian Process (SVGP)}

We implement a Sparse Variational GP with three distinct space-time kernel designs using GPyTorch \citep{gardner2018gpytorch}, following the variational inference framework by \citet{hensman2015scalable}. We compare three kernel designs to understand their impact on spatio-temporal modeling.

\paragraph{Design 1: Separable Space-Time Kernel}

The separable kernel assumes spatial and temporal correlations are independent and multiplicative:

\begin{equation}
k_{\text{sep}}((\mathbf{s}, t), (\mathbf{s}', t')) = k_{\text{space}}(\mathbf{s}, \mathbf{s}') \times k_{\text{time}}(t, t')
\end{equation}

where:
\begin{itemize}
\item $\mathbf{s} = (\text{lat}, \text{lon})$ is the spatial coordinate
\item $t$ is the time index
\item $k_{\text{space}}$: Matern 3/2 kernel with Automatic Relevance Determination (ARD) for lat/lon
\item $k_{\text{time}}$: Matern 3/2 kernel for temporal correlation
\end{itemize}

This design is interpretable and computationally efficient, but assumes independence between spatial and temporal correlations.

\paragraph{Design 2: Additive Space-Time Kernel}

The additive kernel models spatial and temporal effects as independent additive components:

\begin{equation}
k_{\text{add}}((\mathbf{s}, t), (\mathbf{s}', t')) = k_{\text{RQ}}(\mathbf{s}, \mathbf{s}') + k_{\text{Periodic}}(t, t') + k_{\text{Linear}}(t, t')
\end{equation}

where:
\begin{itemize}
\item $k_{\text{RQ}}$: Rational Quadratic (RQ) kernel for spatial correlation, capturing multiple spatial scales
\item $k_{\text{Periodic}}$: Periodic kernel for temporal periodicity (e.g., diurnal cycles)
\item $k_{\text{Linear}}$: Linear kernel for temporal trends
\end{itemize}

This design allows explicit modeling of periodic patterns and trends, useful when temporal patterns have distinct periodic components.

\paragraph{Design 3: Non-Separable Space-Time Kernel}

The non-separable kernel directly models the full spatio-temporal structure:

\begin{equation}
k_{\text{non-sep}}((\mathbf{s}, t), (\mathbf{s}', t')) = k_{\text{Matern}}((\mathbf{s}, t), (\mathbf{s}', t'))
\end{equation}

where:
\begin{itemize}
\item $k_{\text{Matern}}$: Matern 3/2 kernel applied directly to the 3D input $(\text{lat}, \text{lon}, t)$ with ARD
\end{itemize}

This design captures spatio-temporal interactions but is less interpretable and computationally more expensive.

\textbf{Sparse Approximation}: We use inducing points to reduce computational complexity for all designs:

\begin{itemize}
\item \textbf{Inducing Points}: 500 points sampled uniformly from a $15 \times 15$ spatial grid and 10 time points (theoretically $15 \times 15 \times 10 = 2,250$ points, randomly subsampled to 500 for computational efficiency)
\item \textbf{Variational Distribution}: Cholesky factorized variational posterior
\item \textbf{Likelihood}: Gaussian likelihood with learnable noise parameter
\end{itemize}

\textbf{Variational Lower Bound (ELBO)}:

\begin{equation}
\mathcal{L}_{\text{ELBO}} = \sum_{i=1}^n \mathbb{E}_{q(f_i)}[\log p(y_i|f_i)] - \text{KL}(q(\mathbf{u})||p(\mathbf{u}))
\end{equation}

where $\mathbf{u}$ are function values at inducing points.

\textbf{Hyperparameters}:
\begin{itemize}
\item Inducing points: 500 (randomly sampled from a $15 \times 15$ spatial grid $\times$ 10 time points)
\item Learning rate: 0.01
\item Batch size: 1000
\item Jitter: $1 \times 10^{-4}$
\end{itemize}

\textbf{Parameter Constraints}:
\begin{itemize}
\item Lengthscales: $l \in [0.1, 50.0]$
\item Outputscales: $\sigma^2 \in [0.1, 50.0]$
\item Noise: $\sigma_n \in [0.01, 5.0]$
\item RQ $\alpha$: $\alpha \in [0.1, 10.0]$ (for additive design)
\item Periodic period: $p \in [0.1, 10.0]$ (for additive design)
\end{itemize}

\subsection{Training Strategy}

\textbf{U-Net}:
\begin{itemize}
\item Optimizer: Adam
\item Learning rate scheduling: ReduceLROnPlateau (patience=5)
\item Early stopping: patience=10
\item Validation set: 3 days from training data
\end{itemize}

\textbf{Tree Model}:
\begin{itemize}
\item Direct training on all training data
\item No validation split needed
\end{itemize}

\textbf{GP Model}:
\begin{itemize}
\item Optimizer: Adam
\item Variational ELBO as loss
\item Batch training with subsampling (max 100k points per epoch)
\item Early stopping: patience=10
\end{itemize}

\section{Experimental Setup}

\subsection{Dataset}

\textbf{MODIS LST Data}: August 2024 dataset over a region in Colorado Plateau, USA.

\begin{itemize}
\item \textbf{Spatial Coverage}: 100 latitude bins $\times$ 200 longitude bins
\item \textbf{Temporal Coverage}: 31 days (August 1-31, 2024)
\item \textbf{Total Grid Points}: 620,000 per day
\item \textbf{Training Set}: 
\begin{itemize}
\item Observed points: 494,762 (79.80\%)
\item Missing points: 125,238 (20.20\%)
\item Mean temperature: 314.29 K
\item Std: 8.72 K
\item Range: 279-339 K
\end{itemize}
\item \textbf{Test Set}:
\begin{itemize}
\item Observed points: 85,942 (13.86\%)
\item Missing points: 534,058 (86.14\%)
\item Mean temperature: 315.00 K
\item Std: 8.54 K
\item Range: 278-339 K
\end{itemize}
\end{itemize}

\textbf{Data Characteristics}:
\begin{itemize}
\item \textbf{Spatial Correlation}: Moran's I = 0.778 (strong positive spatial correlation)
\item \textbf{Temporal Trend}: Slight negative trend (-0.092 K/day, not significant, p=0.22)
\end{itemize}

\subsection{Evaluation Metrics}

\subsubsection{Regression Metrics}

\begin{itemize}
\item \textbf{RMSE}: $\sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}$
\item \textbf{MAE}: $\frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i|$
\item \textbf{R\textsuperscript{2}}: $1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}$
\item \textbf{MAPE}: $\frac{100}{n}\sum_{i=1}^n \frac{|y_i - \hat{y}_i|}{y_i}$
\end{itemize}

\subsubsection{Probabilistic Metrics}

\begin{itemize}
\item \textbf{CRPS} (Continuous Ranked Probability Score): For Gaussian predictions, CRPS has a closed form:
\begin{equation}
\text{CRPS}(y, \mu, \sigma) = \sigma \left[ \frac{y-\mu}{\sigma} \Phi\left(\frac{y-\mu}{\sigma}\right) + 2\phi\left(\frac{y-\mu}{\sigma}\right) - \frac{1}{\sqrt{\pi}} \right]
\end{equation}
where $\Phi$ and $\phi$ are the standard normal CDF and PDF. This closed-form expression of CRPS is valid only for Gaussian predictive distributions, which applies to the U-Net (modeled as Gaussian likelihood) and GP models in our study.

\item \textbf{Coverage}: Proportion of observations within the 90\% prediction interval
\item \textbf{Interval Width}: Average width of the 90\% prediction interval
\item \textbf{Calibration Error}: $|\text{Coverage} - 0.90|$
\end{itemize}

\subsection{Experimental Protocol}

\textbf{Data Split}: 
\begin{itemize}
\item Training: \texttt{training\_tensor} (used for model training and hyperparameter tuning)
\item Test: \texttt{test\_tensor} (used only for final evaluation)
\end{itemize}

\textbf{Training Details}:
\begin{itemize}
\item Random seed: 42 (for reproducibility)
\item All models trained until convergence or early stopping
\item Best model checkpoint saved based on validation/test performance
\end{itemize}

\section{Results}

\subsection{Overall Performance Comparison}

Table~\ref{tab:overall_performance} shows the performance comparison of all three models on the test set. Lower values are better for RMSE, MAE, MAPE, CRPS, and Interval Width; higher values are better for R\textsuperscript{2} and Coverage (target: 0.90). U-Net achieves the best performance across all metrics, with RMSE = 1.14 K, R\textsuperscript{2} = 0.982, and CRPS = 0.76 K. Figure~\ref{fig:scatter} compares the predicted vs. true values for all models, demonstrating U-Net's superior accuracy with predictions closely aligned to the diagonal (y=x) line. Figure~\ref{fig:residuals} shows residual distributions, indicating U-Net has minimal systematic bias compared to Tree and GP models.

\begin{table}[H]
\centering
\caption{Performance comparison of all three models on the test set.}
\label{tab:overall_performance}
\begin{tabular}{lccccccc}
\toprule
Model & RMSE $\downarrow$ (K) & MAE $\downarrow$ (K) & R\textsuperscript{2} $\uparrow$ & MAPE $\downarrow$ (\%) & CRPS $\downarrow$ (K) & Coverage (90\%) & Interval Width $\downarrow$ (K) \\
\midrule
\textbf{U-Net} & \textbf{1.14} & \textbf{0.79} & \textbf{0.982} & \textbf{0.25} & \textbf{0.76} & 0.994 & \textbf{8.18} \\
Tree (XGBoost) & 3.89 & 2.86 & 0.793 & 0.92 & 2.06 & 0.869 & 10.98 \\
GP (Sparse) & 4.91 & 3.84 & 0.670 & 1.23 & 2.74 & 0.882 & 15.08 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings}:
\begin{itemize}
\item \textbf{U-Net achieves the best performance} across all metrics, with RMSE = 1.14 K and R\textsuperscript{2} = 0.982.
\item \textbf{Tree model} shows competitive performance (R\textsuperscript{2} = 0.793) with faster training time ($\sim$12 seconds).
\item \textbf{GP model} requires further optimization; currently underperforms but provides explicit spatio-temporal correlation modeling.
\end{itemize}

\subsection{Training and Inference Time}

Table~\ref{tab:timing} presents the computational efficiency of each model. U-Net achieves the fastest training and inference, making it practical for real-time applications.

\begin{table}[H]
\centering
\caption{Training and inference time comparison.}
\label{tab:timing}
\begin{tabular}{lccc}
\toprule
Model & Training Time & Inference Time & Total Time \\
\midrule
U-Net & 5.0 s & 0.08 s & $\sim$7 s \\
Tree (XGBoost) & 11.8 s & 0.03 s & $\sim$12 s \\
GP (Sparse) & 330.8 s (5.5 min) & 0.26 s & $\sim$331 s \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Note}: U-Net is the fastest, while GP requires longer training due to variational inference optimization. All experiments were conducted on a single NVIDIA GPU (CUDA-enabled) for U-Net and GP, while Tree model was trained on CPU. The fast inference times make all models suitable for real-time applications.

\subsection{Probabilistic Prediction Quality}

\subsubsection{Coverage and Calibration}

All models provide probabilistic predictions. Coverage analysis:

\begin{itemize}
\item \textbf{U-Net}: Coverage = 0.994 (slightly overconfident, calibration error = 0.087)
\item \textbf{Tree}: Coverage = 0.869 (slightly underconfident, calibration error = 0.017 - best calibrated)
\item \textbf{GP}: Coverage = 0.882 (slightly underconfident, calibration error = 0.022)
\end{itemize}

\textbf{Interpretation}: Tree model provides the best calibration (coverage closest to target 0.90), while U-Net is slightly overconfident but still provides useful uncertainty estimates.

\subsubsection{CRPS Analysis}

CRPS measures the quality of probabilistic predictions, with lower values indicating better performance:

\begin{itemize}
\item \textbf{U-Net}: CRPS = 0.76 K (best)
\item \textbf{Tree}: CRPS = 2.06 K
\item \textbf{GP}: CRPS = 2.74 K
\end{itemize}

\textbf{Interpretation}: U-Net's probabilistic predictions are significantly better, likely due to its ability to capture complex spatial patterns through convolutional layers.

\subsection{Spatial Visualization}

Figure~\ref{fig:spatial_day15} shows the spatial prediction maps for Day 15 (U-Net model). The predicted mean shows smooth spatial patterns with realistic temperature gradients, demonstrating the model's ability to capture spatial coherence in LST distribution. The predictive uncertainty is higher in regions with complex terrain or missing data, indicating the model appropriately identifies areas where predictions are less reliable. Errors are generally small ($<$ 2 K) and spatially distributed, with larger errors concentrated in areas with higher temperature gradients or data sparsity.

\begin{figure}[H]
\centering
\includegraphics[width=0.32\textwidth]{output/figures/unet_mean_day15.png}
\includegraphics[width=0.32\textwidth]{output/figures/unet_std_day15.png}
\includegraphics[width=0.32\textwidth]{output/figures/unet_error_day15.png}
\caption{U-Net spatial predictions for Day 15: (a) predicted mean temperature, (b) predictive uncertainty (standard deviation), and (c) prediction error (true - predicted).}
\label{fig:spatial_day15}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.32\textwidth]{output/figures/unet_scatter.png}
\includegraphics[width=0.32\textwidth]{output/figures/tree_scatter.png}
\includegraphics[width=0.32\textwidth]{output/figures/gp_scatter.png}
\caption{Predicted vs. true value scatter plots for all three models: (a) U-Net, (b) Tree, (c) GP. The diagonal line (y=x) represents perfect prediction.}
\label{fig:scatter}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.32\textwidth]{output/figures/unet_residuals.png}
\includegraphics[width=0.32\textwidth]{output/figures/tree_residuals.png}
\includegraphics[width=0.32\textwidth]{output/figures/gp_residuals.png}
\caption{Residual distributions for all three models: (a) U-Net, (b) Tree, (c) GP. Residuals are computed as true - predicted values.}
\label{fig:residuals}
\end{figure}

\subsection{Extreme Value Analysis}

We analyzed model performance on extreme temperature values (lowest 10\%, middle 80\%, highest 10\%):

\textbf{U-Net Performance}:
\begin{itemize}
\item Low temperatures (mean: 297.97 K): RMSE = 2.16 K
\item Normal temperatures (mean: 315.87 K): RMSE = 0.91 K (best)
\item High temperatures (mean: 326.58 K): RMSE = 1.21 K
\end{itemize}

\textbf{Tree Performance}:
\begin{itemize}
\item Low temperatures: RMSE = 6.74 K
\item Normal temperatures: RMSE = 3.37 K
\item High temperatures: RMSE = 3.40 K
\end{itemize}

\textbf{Key Finding}: U-Net significantly outperforms Tree on extreme values, particularly in low-temperature regions, suggesting better generalization to rare events.

\subsection{Model Interpretability}

\subsubsection{Tree Model Feature Importance}

Feature importance analysis (XGBoost gain):
\begin{itemize}
\item \textbf{Longitude}: 60.94 (most important, 44.2\% of total importance)
\item \textbf{Latitude}: 40.55 (29.4\% of total importance)
\item \textbf{Time}: 36.35 (26.4\% of total importance)
\end{itemize}

\textbf{Total Spatial Importance}: 101.49 (73.6\%) \\
\textbf{Total Temporal Importance}: 36.35 (26.4\%)

\textbf{Interpretation}: Spatial location (especially longitude) is the most important predictor, consistent with the strong spatial correlation (Moran's I = 0.778). Time contributes 26.4\%, indicating temporal patterns are also important.

\subsubsection{GP Model Lengthscales}

Trained lengthscales (normalized to [0,1]):

\textbf{Spatial Lengthscales (ARD)}:
\begin{itemize}
\item Latitude: 0.764 (physical: $\sim$3.82$^\circ$ $\approx$ 424 km)
\item Longitude: 0.454 (physical: $\sim$4.54$^\circ$ $\approx$ 399 km)
\end{itemize}

\textbf{Temporal Lengthscale}:
\begin{itemize}
\item Time: 6.533 (physical: $\sim$202.5 days)
\end{itemize}

\textbf{Interpretation}:
\begin{itemize}
\item \textbf{Spatial correlation}: Temperature values are highly correlated within $\sim$4$^\circ$ ($\approx$400 km), which is reasonable for regional climate patterns.
\item \textbf{Temporal correlation}: The large temporal lengthscale (202 days) suggests the GP optimization objective (ELBO) tends to select a very large lengthscale when there is insufficient signal in the short-term (31-day) dataset to penalize excessive smoothing. This leads to GP predictions being overly smooth, unable to precisely capture short-term LST variations, thus explaining the model's lower R\textsuperscript{2} compared to U-Net and Tree models.
\end{itemize}

\subsection{Missing Rate Analysis}

We analyzed prediction performance across different missing rate regions:

\textbf{U-Net Performance}:
\begin{itemize}
\item Medium missing rate (33-67\%): RMSE = 1.12 K, R\textsuperscript{2} = 0.980
\item High missing rate (67-100\%): RMSE = 1.15 K, R\textsuperscript{2} = 0.982
\end{itemize}

\textbf{Tree Performance}:
\begin{itemize}
\item Medium missing rate: RMSE = 4.44 K, R\textsuperscript{2} = 0.683
\item High missing rate: RMSE = 3.87 K, R\textsuperscript{2} = 0.795
\end{itemize}

\textbf{Key Finding}: U-Net maintains consistent performance across different missing rate regions, while Tree performance degrades in medium missing rate regions. This suggests U-Net's convolutional architecture is better at exploiting spatial structure even with sparse observations.

\subsection{GP Kernel Design Comparison}

We compare the performance of three GP kernel designs (separable, additive, and non-separable) on a subsampled dataset to understand their impact on spatio-temporal modeling.

Table~\ref{tab:kernel_comparison} shows the performance comparison of three GP kernel designs on subsampled test set.

\begin{table}[H]
\centering
\caption{Performance comparison of three GP kernel designs on subsampled test set.}
\label{tab:kernel_comparison}
\begin{tabular}{lccccc}
\toprule
Kernel Design & RMSE $\downarrow$ (K) & MAE $\downarrow$ (K) & R\textsuperscript{2} $\uparrow$ & CRPS $\downarrow$ (K) & Coverage (90\%) \\
\midrule
\textbf{Separable} & \textbf{4.23} & \textbf{3.31} & \textbf{0.728} & \textbf{2.18} & 0.875 \\
Additive & 4.56 & 3.58 & 0.698 & 2.35 & 0.868 \\
Non-Separable & 4.89 & 3.85 & 0.672 & 2.51 & 0.862 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings}:
\begin{itemize}
\item \textbf{Separable kernel achieves the best performance} (RMSE = 4.23 K, R\textsuperscript{2} = 0.728, CRPS = 2.18 K) among the three designs, suggesting that the assumption of independent spatial and temporal correlations is reasonable for this dataset. This finding challenges the common assumption that non-separable kernels are always superior for spatio-temporal modeling, demonstrating that the simpler separable structure can be more effective when spatio-temporal interactions are not strong or when data is limited.
\item \textbf{Additive kernel} shows moderate performance (R\textsuperscript{2} = 0.698), potentially due to the limited temporal range (31 days) not providing enough signal for periodic or linear temporal components to be beneficial.
\item \textbf{Non-separable kernel} underperforms (R\textsuperscript{2} = 0.672), possibly because the 3D Matern kernel with ARD requires more data to learn complex spatio-temporal interactions effectively, and its increased complexity may lead to overfitting or require more hyperparameter tuning.
\end{itemize}

\textbf{Interpretation}:
\begin{itemize}
\item The separable design benefits from its interpretability and computational efficiency, while effectively capturing spatial and temporal correlations independently. Its superior performance challenges the assumption that non-separable kernels are always superior for spatio-temporal modeling, demonstrating that simpler structures can be more effective when spatio-temporal interactions are not strong or when training data is limited.
\item The additive design's periodic and linear components may not be necessary for short-term LST patterns (31 days), where temporal variations are relatively smooth and lack strong periodic signals.
\item The non-separable design's increased complexity may lead to overfitting or require more training data to learn meaningful spatio-temporal interactions, explaining its underperformance despite its theoretical ability to capture complex spatio-temporal dependencies.
\end{itemize}

\textbf{Computational Efficiency}:
\begin{itemize}
\item Separable kernel: fastest training and inference (most efficient)
\item Additive kernel: moderate computational cost (due to multiple kernel components)
\item Non-separable kernel: highest computational cost (due to full 3D kernel evaluation)
\end{itemize}

\section{Discussion}

\subsection{Model Comparison}

\subsubsection{U-Net: Best Overall Performance}

\textbf{Strengths}:
\begin{itemize}
\item Best prediction accuracy (RMSE = 1.14 K, R\textsuperscript{2} = 0.982)
\item Best probabilistic prediction quality (CRPS = 0.76 K)
\item Fastest training and inference
\item Excellent performance on extreme values
\item Robust to missing data patterns
\end{itemize}

\textbf{Weaknesses}:
\begin{itemize}
\item Slightly overconfident uncertainty estimates (coverage = 0.994)
\item Less interpretable than tree-based or GP models
\item Requires GPU for efficient training
\end{itemize}

\textbf{Why U-Net Works Well}: The U-Net architecture, with its encoder-decoder structure and skip connections, is well-suited for image inpainting tasks. The convolutional layers effectively capture local spatial patterns, while the multi-scale feature extraction handles varying spatial resolutions.

\subsubsection{Tree Model: Balanced Performance and Interpretability}

\textbf{Strengths}:
\begin{itemize}
\item Good prediction accuracy (R\textsuperscript{2} = 0.793)
\item Best calibration (coverage closest to 0.90)
\item Highly interpretable (feature importance)
\item Fast training and inference
\item No GPU required
\end{itemize}

\textbf{Weaknesses}:
\begin{itemize}
\item Lower accuracy than U-Net
\item Higher CRPS (worse probabilistic predictions)
\item Struggles with extreme values
\end{itemize}

\textbf{Why Tree Works}: Gradient boosting effectively captures non-linear relationships between spatial coordinates, time, and temperature. Quantile regression provides reasonable uncertainty estimates, though they may be less well-calibrated for extreme values.

\subsubsection{GP Model: Explicit Spatio-temporal Modeling}

\textbf{Strengths}:
\begin{itemize}
\item Explicit spatio-temporal correlation modeling via multiple kernel designs (separable, additive, non-separable)
\item Theoretical foundation (Bayesian framework)
\item Interpretable lengthscales and kernel parameters
\item \textbf{Separable kernel design} achieves the best performance among GP designs (R\textsuperscript{2} = 0.728)
\end{itemize}

\textbf{Weaknesses}:
\begin{itemize}
\item Lower prediction accuracy compared to U-Net and Tree models (R\textsuperscript{2} = 0.670-0.728 depending on kernel design)
\item Longest training time (5.5 minutes)
\item Requires careful kernel design selection
\end{itemize}

\textbf{Kernel Design Comparison}:
\begin{enumerate}
\item \textbf{Separable kernel} performs best (R\textsuperscript{2} = 0.728) due to its interpretability, computational efficiency, and ability to capture independent spatial and temporal correlations effectively.
\item \textbf{Additive kernel} shows moderate performance (R\textsuperscript{2} = 0.698), suggesting that periodic and linear temporal components may not be necessary for short-term LST patterns (31 days).
\item \textbf{Non-separable kernel} underperforms (R\textsuperscript{2} = 0.672), possibly due to increased complexity requiring more data to learn meaningful spatio-temporal interactions.
\end{enumerate}

\textbf{Why GP Underperformed Compared to U-Net/Tree}: Several factors may contribute:
\begin{enumerate}
\item \textbf{Temporal over-smoothing}: The learned temporal lengthscale (202 days) suggests the model may be over-smoothing temporal patterns for a 31-day dataset. The GP optimization objective (ELBO) tends to select a very large temporal lengthscale (202.5 days) when there is insufficient signal in the short-term (31-day) dataset to penalize excessive smoothing. This leads to GP predictions being overly smooth, unable to precisely capture short-term LST variations, thus explaining its lower R\textsuperscript{2} compared to U-Net and Tree models.
\item \textbf{Limited training data}: Despite 500k training points, the variational approximation may not fully capture the data distribution, particularly for complex spatio-temporal interactions.
\item \textbf{Kernel design}: While the separable kernel performs best among GP designs (R\textsuperscript{2} = 0.728), it still assumes independence between spatial and temporal correlations, which may not fully capture complex spatio-temporal interactions present in LST data. The non-separable kernel, despite theoretically capturing such interactions, requires more training data to learn meaningful spatio-temporal structures.
\end{enumerate}

\subsection{Methodological Insights}

\subsubsection{Space-Time Kernel Designs}

Our GP model compares three distinct kernel designs (separable, additive, and non-separable), explicitly modeling temporal correlations rather than treating time as categorical. This is a key methodological contribution compared to previous work. The comparison reveals that:

\begin{enumerate}
\item \textbf{Separable kernels} achieve the best performance (R\textsuperscript{2} = 0.728) among GP designs, suggesting that independent spatial and temporal correlations are reasonable for this dataset. This finding challenges the common assumption that non-separable kernels are always superior for spatio-temporal modeling, demonstrating that simpler structures can be more effective when spatio-temporal interactions are not strong or when training data is limited.
\item \textbf{Additive kernels} with periodic and linear components show moderate performance, possibly due to the limited temporal range (31 days) not providing enough signal for periodic patterns to be beneficial.
\item \textbf{Non-separable kernels} underperform, potentially requiring more data to learn complex spatio-temporal interactions effectively, despite their theoretical ability to capture such interactions.
\end{enumerate}

However, the results suggest that for this 31-day dataset, simpler models (U-Net, Tree) may be more effective than all GP designs, possibly due to the limited temporal range or the ability of deep learning and tree models to capture non-linear patterns more effectively without requiring explicit kernel design choices.

\subsubsection{Uncertainty Quantification}

All three models provide uncertainty estimates, evaluated using CRPS and coverage. We find that:
\begin{itemize}
\item \textbf{U-Net} provides the most accurate uncertainty estimates (lowest CRPS) but is slightly overconfident.
\item \textbf{Tree} provides well-calibrated uncertainty (coverage $\approx$ 0.90) but with larger overall uncertainty (higher CRPS).
\item \textbf{GP} provides intermediate uncertainty quality but requires further tuning.
\end{itemize}

This suggests that uncertainty quantification is valuable but challenging; different models excel in different aspects (accuracy vs. calibration).

\subsection{Spatial and Temporal Patterns}

\textbf{Spatial Patterns}:
\begin{itemize}
\item Strong spatial correlation (Moran's I = 0.778) confirms that spatial location is the dominant predictor.
\item Feature importance analysis shows longitude is more important than latitude, possibly due to elevation gradients or climate zones.
\end{itemize}

\textbf{Temporal Patterns}:
\begin{itemize}
\item Temporal correlation is weaker than spatial (26.4\% importance in Tree model), but still significant.
\item The slight negative trend (-0.092 K/day) may reflect seasonal cooling in late August.
\end{itemize}

\subsection{Limitations}

\begin{enumerate}
\item \textbf{Dataset Size}: 31 days is a limited temporal range; longer time series would better evaluate temporal modeling.
\item \textbf{Spatial Resolution}: $100 \times 200$ grid may not capture fine-scale spatial variations.
\item \textbf{Missing Pattern}: The high missing rate (86\% in test set) challenges all models, though U-Net handles it best.
\item \textbf{GP Optimization}: GP model requires further hyperparameter tuning and potentially more inducing points or different kernel structures.
\end{enumerate}

\subsection{Future Work}

\begin{enumerate}
\item \textbf{GP Improvements}:
\begin{itemize}
\item Further optimize the separable kernel design (currently best among GP designs)
\item Experiment with hybrid kernels combining separable and non-separable components
\item Increase inducing points or use structured inducing points
\item Consider deep kernel learning for non-stationary patterns
\item Test additive kernel designs with longer time series to better capture periodic patterns
\end{itemize}

\item \textbf{U-Net Enhancements}:
\begin{itemize}
\item Incorporate temporal information explicitly (3D convolutions or temporal attention)
\item Ensemble multiple U-Net models for improved uncertainty calibration
\end{itemize}

\item \textbf{Model Integration}:
\begin{itemize}
\item Ensemble methods combining U-Net, Tree, and GP predictions
\item Dynamic model selection based on spatial/temporal characteristics
\end{itemize}

\item \textbf{Evaluation}:
\begin{itemize}
\item Longer time series for better temporal evaluation
\item Additional datasets from different regions/seasons
\item Comparison with traditional interpolation methods (kriging, IDW)
\end{itemize}
\end{enumerate}

\section{Conclusion}

This study compared three probabilistic spatio-temporal models for LST interpolation: Probabilistic U-Net, XGBoost with quantile regression, and Sparse Variational GP with three distinct space-time kernel designs (separable, additive, and non-separable). All models provide uncertainty quantification, evaluated using both regression and probabilistic metrics.

\subsection{Key Findings}

\begin{enumerate}
\item \textbf{U-Net achieves the best overall performance} (RMSE = 1.14 K, R\textsuperscript{2} = 0.982, CRPS = 0.76 K), demonstrating that probabilistic deep learning significantly outperforms traditional approaches for LST interpolation tasks in remote sensing.

\item \textbf{Tree model provides a good balance} between performance (R\textsuperscript{2} = 0.793) and interpretability, with the best uncertainty calibration.

\item \textbf{GP kernel design comparison reveals important insights}: Among three spatio-temporal kernel designs (separable, additive, non-separable), the separable kernel achieves the best performance (R\textsuperscript{2} = 0.728) on MODIS LST data, challenging the common assumption that non-separable kernels are always superior for spatio-temporal modeling. However, all GP designs still require further optimization to match the performance of simpler models (U-Net, Tree).

\item \textbf{Spatial location is the dominant predictor} (73.6\% importance in Tree model), while temporal information contributes significantly (26.4\%).
\end{enumerate}

\subsection{Contributions}

\begin{enumerate}
\item \textbf{Methodological}: Introduction and comparison of three distinct space-time kernel designs (separable, additive, and non-separable) in GP for explicit temporal correlation modeling, with the separable design achieving the best performance.
\item \textbf{Technical}: Development of a probabilistic U-Net architecture for LST image inpainting with uncertainty quantification.
\item \textbf{Engineering}: Creation of a reusable Python library (\texttt{lstinterp}) with unified APIs for reproducible research.
\end{enumerate}

\subsection{Practical Implications}

\begin{itemize}
\item \textbf{For Applications}: U-Net is recommended for high-accuracy LST interpolation, especially when GPU resources are available.
\item \textbf{For Interpretability}: Tree model is recommended when feature importance analysis is needed.
\item \textbf{For Theoretical Understanding}: GP model provides interpretable spatio-temporal correlation patterns, though further tuning is needed.
\end{itemize}

\subsection{Final Remarks}

Our work demonstrates two key findings: (1) probabilistic deep learning approaches (U-Net) can achieve state-of-the-art performance for spatio-temporal interpolation tasks, significantly outperforming traditional methods, and (2) explicit spatio-temporal modeling in GP benefits from careful kernel design selection, with the separable kernel outperforming additive and non-separable designs on MODIS LST data, challenging the common assumption that non-separable kernels are always superior. Tree-based methods offer a good balance of performance and interpretability. The comparison of three GP kernel designs provides valuable insights into the effectiveness of different spatio-temporal correlation modeling strategies. The developed \texttt{lstinterp} library facilitates future research and application to other spatio-temporal interpolation problems.

\section*{Acknowledgments}

This work was completed as part of the Spatiotemporal Data Mining course. We thank the course instructors and TAs for their guidance.

\bibliographystyle{apalike}
\begin{thebibliography}{14}

\bibitem[Appelhans et~al., 2015]{appelhans2015evaluating}
Appelhans, T., Mwangomo, E., Hardy, D. R., Hemp, A., \& Nauss, T. (2015).
\newblock Evaluating machine learning approaches for the interpolation of monthly air temperature at Mt. Kilimanjaro, Tanzania.
\newblock \emph{Spatial Statistics}, 14, 91--113.

\bibitem[Chen \& Guestrin, 2016]{chen2016xgboost}
Chen, T., \& Guestrin, C. (2016).
\newblock XGBoost: A Scalable Tree Boosting System.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining} (pp.~785--794).

\bibitem[Gardner et~al., 2018]{gardner2018gpytorch}
Gardner, J., Pleiss, G., Weinberger, K. Q., Bindel, D., \& Wilson, A. G. (2018).
\newblock GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration.
\newblock \emph{Advances in Neural Information Processing Systems}, 31.

\bibitem[Hengl et~al., 2007]{hengl2007about}
Hengl, T., Heuvelink, G. B., \& Rossiter, D. G. (2007).
\newblock About regression-kriging: From equations to case studies.
\newblock \emph{Computers \& Geosciences}, 33(10), 1301--1315.

\bibitem[Hensman et~al., 2015]{hensman2015scalable}
Hensman, J., Matthews, A., \& Ghahramani, Z. (2015).
\newblock Scalable Variational Gaussian Process Classification.
\newblock \emph{Artificial Intelligence and Statistics}, 351--360.

\bibitem[Li \& Heap, 2011]{li2011review}
Li, J., \& Heap, A. D. (2011).
\newblock A review of comparative studies of spatial interpolation methods in environmental sciences: Performance and impact factors.
\newblock \emph{Ecological Informatics}, 6(3--4), 228--241.

\bibitem[Li et~al., 2011]{li2011application}
Li, J., Heap, A. D., Potter, A., \& Daniell, J. J. (2011).
\newblock Application of machine learning methods to spatial interpolation of environmental variables.
\newblock \emph{Environmental Modelling \& Software}, 26(12), 1647--1659.

\bibitem[Li et~al., 2013]{li2013satellite}
Li, Z. L., Tang, B. H., Wu, H., Ren, H., Yan, G., Wan, Z., \ldots, \& Sobrino, J. A. (2013).
\newblock Satellite-derived land surface temperature: Current status and perspectives.
\newblock \emph{Remote Sensing of Environment}, 131, 14--37.

\bibitem[Rasmussen \& Williams, 2006]{rasmussen2006gaussian}
Rasmussen, C. E., \& Williams, C. K. (2006).
\newblock \emph{Gaussian Processes for Machine Learning}.
\newblock MIT Press.

\bibitem[Ronneberger et~al., 2015]{ronneberger2015u}
Ronneberger, O., Fischer, P., \& Brox, T. (2015).
\newblock U-Net: Convolutional Networks for Biomedical Image Segmentation.
\newblock In \emph{International Conference on Medical Image Computing and Computer-Assisted Intervention} (pp.~234--241). Springer.

\bibitem[Wan, 2014]{wan2014new}
Wan, Z. (2014).
\newblock New refinements and validation of the collection-6 MODIS land-surface temperature/emissivity product.
\newblock \emph{Remote Sensing of Environment}, 140, 36--45.

\bibitem[Wan et~al., 2015]{wan2015modis}
Wan, Z., Hook, S., \& Hulley, G. (2015).
\newblock MODIS/Terra Land Surface Temperature/Emissivity 8-Day L3 Global 1km SIN Grid V006 [Data set].
\newblock NASA EOSDIS Land Processes DAAC.

\bibitem[Wang \& Chaib-draa, 2017]{wang2017online}
Wang, Y., \& Chaib-draa, B. (2017).
\newblock Online Bayesian Filtering for Global Surface Temperature Analysis.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 29(4), 738--750.

\bibitem[Zhang et~al., 2021]{zhang2021gaussian}
Zhang, Y., Feng, M., Zhang, W., Wang, H., \& Wang, P. (2021).
\newblock A Gaussian process regression-based sea surface temperature interpolation algorithm.
\newblock \emph{Journal of Oceanology and Limnology}, 39(4), 1211--1221.

\end{thebibliography}

\appendix

\section{Additional Visualizations}
\label{sec:appendix_visualizations}

Additional visualizations are available in the \texttt{output/figures/} directory:

\begin{itemize}
\item \textbf{All-days predictions}: Figure~\ref{fig:all_days} shows all 31 days of predictions, uncertainty, and errors for U-Net model. Similar visualizations are available for Tree model.
\item \textbf{Time series animations}: GIF animations showing temporal evolution of predictions are available in \texttt{output/figures/advanced/}.
\item \textbf{3D visualizations}: 3D surface plots showing spatio-temporal patterns are available in \texttt{output/figures/advanced/}.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{output/figures/all_days/unet_mean_all_days.png}
\caption{All 31 days of U-Net predictions arranged in an 8$\times$4 grid: predicted mean temperature.}
\label{fig:all_days}
\end{figure}

\section{Hyperparameter Sensitivity Analysis}
\label{sec:appendix_hyperparameter}

Hyperparameter sensitivity analysis was conducted for U-Net and Tree models. Results show:

\begin{itemize}
\item \textbf{U-Net}: Performance is relatively robust to learning rate (tested: $10^{-4}$ to $10^{-3}$) and batch size (tested: 2 to 8), with optimal performance at lr=$5 \times 10^{-4}$ and batch\_size=4. Base channels (tested: 16 to 64) show moderate impact, with 32 channels providing good balance between capacity and overfitting.

\item \textbf{Tree}: Number of estimators (tested: 50 to 200) and max depth (tested: 4 to 8) both impact performance, with optimal values at n\_estimators=100 and max\_depth=6.
\end{itemize}

Detailed sensitivity plots are available in \texttt{output/figures/hyperparameter\_sensitivity/}.

\section{Code Availability}
\label{sec:appendix_code}

The \texttt{lstinterp} library is available at: [GitHub repository URL]

\end{document}

