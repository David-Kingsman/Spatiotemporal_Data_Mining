"""
è®­ç»ƒæ—¶ç©ºé«˜æ–¯è¿‡ç¨‹æ¨¡å‹ï¼ˆSpatio-temporal Gaussian Processï¼‰

æœ¬è„šæœ¬å®ç°äº†åŸºäºæ—¶ç©ºå¯åˆ†æ ¸ï¼ˆseparable spatio-temporal kernelï¼‰çš„ç¨€ç–é«˜æ–¯è¿‡ç¨‹æ¨¡å‹ï¼Œ
ç”¨äºMODISåœ°è¡¨æ¸©åº¦æ•°æ®çš„æ’å€¼å’Œé¢„æµ‹ã€‚

ä¸»è¦ç‰¹ç‚¹ï¼š
1. æ—¶ç©ºå¯åˆ†æ ¸ï¼šk(x, x') = k_space(lat, lon) Ã— k_time(t)
   - ç©ºé—´æ ¸ï¼šMatern 3/2ï¼ˆæ•è·ç©ºé—´ç›¸å…³æ€§ï¼‰
   - æ—¶é—´æ ¸ï¼šMatern 3/2ï¼ˆæ•è·æ—¶é—´ç›¸å…³æ€§ï¼‰
2. ç¨€ç–GPï¼šä½¿ç”¨è¯±å¯¼ç‚¹ï¼ˆinducing pointsï¼‰æé«˜å¯æ‰©å±•æ€§
3. å˜åˆ†æ¨ç†ï¼šä½¿ç”¨Variational ELBOè¿›è¡Œé«˜æ•ˆè®­ç»ƒ
4. æ¦‚ç‡é¢„æµ‹ï¼šæä¾›é¢„æµ‹å‡å€¼å’Œä¸ç¡®å®šæ€§ä¼°è®¡

æ•°æ®æ ¼å¼ï¼š
- è¾“å…¥ï¼š3ç»´å¼ é‡ (H, W, T) = (100, 200, 31)
  - H: çº¬åº¦ç»´åº¦ï¼ˆ35Â°-40Â°Nï¼‰
  - W: ç»åº¦ç»´åº¦ï¼ˆ-115Â°--105Â°Wï¼‰
  - T: æ—¶é—´ç»´åº¦ï¼ˆ31å¤©ï¼‰
- è¾“å‡ºï¼šæ¸©åº¦å€¼ï¼ˆå•ä½ï¼šKelvinï¼‰
- ç¼ºå¤±å€¼ï¼šç”¨0è¡¨ç¤º

è¯„ä¼°æŒ‡æ ‡ï¼š
- å›å½’æŒ‡æ ‡ï¼šRMSE, MAE, RÂ², MAPE
- æ¦‚ç‡æŒ‡æ ‡ï¼šCRPS, 90%é¢„æµ‹åŒºé—´è¦†ç›–ç‡, æ ¡å‡†è¯¯å·®

ä½œè€…ï¼šlstinterpå›¢é˜Ÿ
åˆ›å»ºæ—¶é—´ï¼š2024å¹´
"""
import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import sys
import os
from pathlib import Path
import json
import time
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from lstinterp.data import load_modis_tensor, MODISDataset
from lstinterp.models import GPSTModel, GPSTConfig
from lstinterp.metrics import compute_regression_metrics, compute_probabilistic_metrics
from lstinterp.viz import plot_prediction_scatter, plot_residuals
from lstinterp.utils import set_seed

# åˆ›å»ºè¾“å‡ºç›®å½•
OUTPUT_DIR = Path("output")
OUTPUT_DIR.mkdir(exist_ok=True)
(OUTPUT_DIR / "results").mkdir(exist_ok=True)
(OUTPUT_DIR / "figures").mkdir(exist_ok=True)
(OUTPUT_DIR / "models").mkdir(exist_ok=True)


def print_section_header(title, width=80):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * width)
    print(f"  {title}")
    print("=" * width)


def print_data_statistics(tensor, name, mode="point"):
    """æ‰“å°è¯¦ç»†çš„æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    print_section_header(f"{name} æ•°æ®ç»Ÿè®¡")
    
    H, W, T = tensor.shape
    print(f"æ•°æ®ç»´åº¦: {H} Ã— {W} Ã— {T}")
    print(f"  - çº¬åº¦ç»´åº¦ (H): {H} ä¸ªç½‘æ ¼ç‚¹ï¼ŒèŒƒå›´: 35Â°N - 40Â°N")
    print(f"  - ç»åº¦ç»´åº¦ (W): {W} ä¸ªç½‘æ ¼ç‚¹ï¼ŒèŒƒå›´: -115Â°W - -105Â°W")
    print(f"  - æ—¶é—´ç»´åº¦ (T): {T} å¤©ï¼ˆ2020å¹´8æœˆï¼‰")
    
    # ç¼ºå¤±å€¼ç»Ÿè®¡
    mask = (tensor != 0.0)
    total_points = H * W * T
    observed_points = mask.sum()
    missing_points = total_points - observed_points
    missing_ratio = missing_points / total_points * 100
    
    print(f"\nç¼ºå¤±å€¼ç»Ÿè®¡:")
    print(f"  - æ€»ç½‘æ ¼ç‚¹æ•°: {total_points:,}")
    print(f"  - è§‚æµ‹ç‚¹æ•°: {observed_points:,} ({observed_points/total_points*100:.2f}%)")
    print(f"  - ç¼ºå¤±ç‚¹æ•°: {missing_points:,} ({missing_ratio:.2f}%)")
    
    # æ¸©åº¦ç»Ÿè®¡
    observed_values = tensor[mask]
    print(f"\næ¸©åº¦ç»Ÿè®¡ (Kelvin):")
    print(f"  - å‡å€¼: {observed_values.mean():.2f} K")
    print(f"  - æ ‡å‡†å·®: {observed_values.std():.2f} K")
    print(f"  - æœ€å°å€¼: {observed_values.min():.2f} K")
    print(f"  - æœ€å¤§å€¼: {observed_values.max():.2f} K")
    print(f"  - ä¸­ä½æ•°: {np.median(observed_values):.2f} K")
    
    # æ¯å¤©ç¼ºå¤±å€¼ç»Ÿè®¡
    missing_per_day = []
    for t in range(T):
        day_mask = (tensor[:, :, t] != 0.0)
        missing_per_day.append((H * W - day_mask.sum()) / (H * W) * 100)
    
    print(f"\næ¯æ—¥ç¼ºå¤±å€¼æ¯”ç‡:")
    print(f"  - å¹³å‡ç¼ºå¤±ç‡: {np.mean(missing_per_day):.2f}%")
    print(f"  - æœ€å°ç¼ºå¤±ç‡: {np.min(missing_per_day):.2f}% (ç¬¬{np.argmin(missing_per_day)+1}å¤©)")
    print(f"  - æœ€å¤§ç¼ºå¤±ç‡: {np.max(missing_per_day):.2f}% (ç¬¬{np.argmax(missing_per_day)+1}å¤©)")


def main():
    """ä¸»å‡½æ•°ï¼šè®­ç»ƒå’Œè¯„ä¼°GPæ¨¡å‹"""
    start_time = time.time()
    experiment_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    print_section_header("å®éªŒé…ç½®", width=80)
    print(f"å®éªŒæ—¶é—´: {experiment_time}")
    print(f"éšæœºç§å­: 42")
    
    set_seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"è®¡ç®—è®¾å¤‡: {device}")
    if device.type == "cuda":
        print(f"  - GPUåç§°: {torch.cuda.get_device_name(0)}")
        print(f"  - GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    
    # æ£€æŸ¥ä¾èµ–åº“
    print("\nä¾èµ–åº“æ£€æŸ¥:")
    try:
        import gpytorch
        print(f"  âœ… GPyTorch: {gpytorch.__version__}")
    except ImportError:
        print("  âŒ é”™è¯¯: éœ€è¦å®‰è£… gpytorch")
        print("  è¯·è¿è¡Œ: pip install gpytorch")
        return
    
    try:
        import numpy as np
        print(f"  âœ… NumPy: {np.__version__}")
    except ImportError:
        print("  âŒ NumPyæœªå®‰è£…")
        return
    
    # åŠ è½½æ•°æ®
    print_section_header("æ•°æ®åŠ è½½")
    data_path = "modis_aug_data/MODIS_Aug.mat"
    print(f"æ•°æ®è·¯å¾„: {data_path}")
    
    print("\nåŠ è½½è®­ç»ƒæ•°æ®...")
    train_tensor = load_modis_tensor(data_path, "training_tensor")
    print_data_statistics(train_tensor, "è®­ç»ƒé›†")
    
    print("\nåŠ è½½æµ‹è¯•æ•°æ®...")
    test_tensor = load_modis_tensor(data_path, "test_tensor")
    print_data_statistics(test_tensor, "æµ‹è¯•é›†")
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆpointæ¨¡å¼ï¼‰
    print_section_header("æ•°æ®é¢„å¤„ç†")
    print("è½¬æ¢ä¸ºç‚¹æ•°æ®æ ¼å¼ (lat, lon, time) â†’ temperature")
    
    print("\nåˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
    train_dataset = MODISDataset(train_tensor, mode="point")
    print(f"  - è®­ç»ƒè§‚æµ‹ç‚¹æ•°: {len(train_dataset):,}")
    
    print("\nåˆ›å»ºæµ‹è¯•æ•°æ®é›†...")
    test_dataset = MODISDataset(test_tensor, mode="point")
    print(f"  - æµ‹è¯•è§‚æµ‹ç‚¹æ•°: {len(test_dataset):,}")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    print("\næå–è®­ç»ƒæ•°æ®...")
    X_train = np.array([train_dataset[i][0].numpy() for i in range(len(train_dataset))])
    y_train = np.array([train_dataset[i][1].numpy() for i in range(len(train_dataset))])
    
    print(f"  - è¾“å…¥ç‰¹å¾ç»´åº¦: {X_train.shape}")
    print(f"    * ç‰¹å¾1 (çº¬åº¦): èŒƒå›´ [{X_train[:, 0].min():.2f}, {X_train[:, 0].max():.2f}]")
    print(f"    * ç‰¹å¾2 (ç»åº¦): èŒƒå›´ [{X_train[:, 1].min():.2f}, {X_train[:, 1].max():.2f}]")
    print(f"    * ç‰¹å¾3 (æ—¶é—´): èŒƒå›´ [{X_train[:, 2].min():.0f}, {X_train[:, 2].max():.0f}] å¤©")
    print(f"  - ç›®æ ‡å˜é‡ç»´åº¦: {y_train.shape}")
    print(f"    * æ¸©åº¦èŒƒå›´: [{y_train.min():.2f}, {y_train.max():.2f}] K")
    print(f"    * æ¸©åº¦å‡å€¼: {y_train.mean():.2f} K")
    print(f"    * æ¸©åº¦æ ‡å‡†å·®: {y_train.std():.2f} K")
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    print("\næå–æµ‹è¯•æ•°æ®...")
    X_test = np.array([test_dataset[i][0].numpy() for i in range(len(test_dataset))])
    y_test = np.array([test_dataset[i][1].numpy() for i in range(len(test_dataset))])
    
    print(f"  - è¾“å…¥ç‰¹å¾ç»´åº¦: {X_test.shape}")
    print(f"    * ç‰¹å¾1 (çº¬åº¦): èŒƒå›´ [{X_test[:, 0].min():.2f}, {X_test[:, 0].max():.2f}]")
    print(f"    * ç‰¹å¾2 (ç»åº¦): èŒƒå›´ [{X_test[:, 1].min():.2f}, {X_test[:, 1].max():.2f}]")
    print(f"    * ç‰¹å¾3 (æ—¶é—´): èŒƒå›´ [{X_test[:, 2].min():.0f}, {X_test[:, 2].max():.0f}] å¤©")
    print(f"  - ç›®æ ‡å˜é‡ç»´åº¦: {y_test.shape}")
    print(f"    * æ¸©åº¦èŒƒå›´: [{y_test.min():.2f}, {y_test.max():.2f}] K")
    print(f"    * æ¸©åº¦å‡å€¼: {y_test.mean():.2f} K")
    print(f"    * æ¸©åº¦æ ‡å‡†å·®: {y_test.std():.2f} K")
    
    # è½¬æ¢ä¸ºtensor
    print("\nè½¬æ¢ä¸ºPyTorchå¼ é‡...")
    X_train = torch.FloatTensor(X_train).to(device)
    y_train = torch.FloatTensor(y_train).to(device)
    X_test = torch.FloatTensor(X_test).to(device)
    y_test_np = y_test.copy()
    print(f"  - æ•°æ®ç±»å‹: {X_train.dtype}")
    print(f"  - è®¾å¤‡: {device}")
    
    # é…ç½®æ¨¡å‹
    print_section_header("æ¨¡å‹é…ç½®")
    config = GPSTConfig(
        kernel_space="matern32",  # ç©ºé—´æ ¸ï¼šMatern 3/2
        kernel_time="matern32",   # æ—¶é—´æ ¸ï¼šMatern 3/2
        num_inducing=500,         # è¯±å¯¼ç‚¹æ•°é‡ï¼ˆæ§åˆ¶æ¨¡å‹å¤æ‚åº¦ï¼‰
        lr=0.01,                  # å­¦ä¹ ç‡
        num_epochs=50,            # è®­ç»ƒè½®æ•°
        batch_size=1000           # æ‰¹å¤§å°
    )
    
    print("æ¨¡å‹è¶…å‚æ•°:")
    print(f"  - ç©ºé—´æ ¸å‡½æ•°: {config.kernel_space} (Matern 3/2)")
    print(f"  - æ—¶é—´æ ¸å‡½æ•°: {config.kernel_time} (Matern 3/2)")
    print(f"  - è¯±å¯¼ç‚¹æ•°é‡: {config.num_inducing}")
    print(f"  - å­¦ä¹ ç‡: {config.lr}")
    print(f"  - è®­ç»ƒè½®æ•°: {config.num_epochs}")
    print(f"  - æ‰¹å¤§å°: {config.batch_size}")
    
    print("\nåˆ›å»ºè¯±å¯¼ç‚¹...")
    # åˆ›å»ºè¯±å¯¼ç‚¹ï¼ˆä½¿ç”¨è®­ç»ƒæ•°æ®çš„ä¸€ä¸ªå­é›†ï¼‰
    from lstinterp.models.gp_st import create_inducing_points
    n_space = 15  # 15Ã—15 = 225 ä¸ªç©ºé—´ç‚¹
    n_time = 10   # 10 ä¸ªæ—¶é—´ç‚¹
    print(f"  - ç©ºé—´ç½‘æ ¼: {n_space}Ã—{n_space} = {n_space**2} ä¸ªç‚¹")
    print(f"  - æ—¶é—´ç‚¹: {n_time} ä¸ªç‚¹")
    print(f"  - ç†è®ºè¯±å¯¼ç‚¹æ€»æ•°: {n_space**2 * n_time:,} ä¸ªç‚¹")
    
    inducing_points = create_inducing_points(
        n_space=n_space,
        n_time=n_time,
        normalize=True
    ).float().to(device)  # è½¬æ¢ä¸ºfloat32ä»¥åŒ¹é…è®­ç»ƒæ•°æ®
    
    print(f"  - å®é™…è¯±å¯¼ç‚¹æ•°é‡: {len(inducing_points):,}")
    
    # å¦‚æœè¯±å¯¼ç‚¹æ•°é‡è¶…è¿‡é…ç½®ï¼Œä½¿ç”¨éšæœºé‡‡æ ·
    if len(inducing_points) > config.num_inducing:
        print(f"  - è¯±å¯¼ç‚¹è¿‡å¤šï¼Œéšæœºé‡‡æ ·è‡³ {config.num_inducing} ä¸ª")
        indices = torch.randperm(len(inducing_points))[:config.num_inducing]
        inducing_points = inducing_points[indices]
        print(f"  - æœ€ç»ˆè¯±å¯¼ç‚¹æ•°é‡: {len(inducing_points)}")
    else:
        print(f"  - ä½¿ç”¨å…¨éƒ¨è¯±å¯¼ç‚¹: {len(inducing_points)}")
    
    print("\nåˆ›å»ºæ¨¡å‹...")
    model = GPSTModel(inducing_points, config).to(device)
    model = model.float()  # ç¡®ä¿æ¨¡å‹ä¹Ÿæ˜¯float32
    
    # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  - æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
    print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # æ¨¡å‹ç»“æ„è¯´æ˜
    print("\næ¨¡å‹ç»“æ„:")
    print("  - GPç±»å‹: Sparse Variational GP (SVGP)")
    print("  - æ ¸å‡½æ•°: æ—¶ç©ºå¯åˆ†æ ¸ k(x, x') = k_space(lat, lon) Ã— k_time(t)")
    print("  - å˜åˆ†åˆ†å¸ƒ: CholeskyVariationalDistribution")
    print("  - å˜åˆ†ç­–ç•¥: VariationalStrategy (learn_inducing_locations=True)")
    print("  - å‡å€¼å‡½æ•°: ConstantMean")
    print("  - ä¼¼ç„¶å‡½æ•°: GaussianLikelihood")
    
    # è®­ç»ƒ
    print_section_header("æ¨¡å‹è®­ç»ƒ")
    model.train()
    model.likelihood.train()
    
    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)
    print(f"ä¼˜åŒ–å™¨: Adam")
    print(f"  - å­¦ä¹ ç‡: {config.lr}")
    
    # ä½¿ç”¨marginal log likelihoodä½œä¸ºæŸå¤±
    # VariationalELBOéœ€è¦æ¥æ”¶GPå¯¹è±¡ï¼ˆmodel.gpï¼‰ï¼Œè€Œä¸æ˜¯åŒ…è£…å™¨
    mll = gpytorch.mlls.VariationalELBO(
        model.likelihood, 
        model.gp,  # ä½¿ç”¨GPå¯¹è±¡è€Œä¸æ˜¯åŒ…è£…å™¨
        num_data=len(X_train)
    )
    print(f"æŸå¤±å‡½æ•°: Variational ELBO")
    print(f"  - æ•°æ®é‡: {len(X_train):,} ä¸ªç‚¹")
    
    best_loss = float('inf')
    best_model_state = None
    best_epoch = 1  # åˆå§‹åŒ–ä¸ºç¬¬ä¸€ä¸ªepoch
    train_losses = []
    training_start_time = time.time()
    
    print(f"\nå¼€å§‹è®­ç»ƒ ({config.num_epochs} ä¸ªepoch)...")
    print("-" * 80)
    print(f"{'Epoch':<8} {'Loss':<15} {'æœ€ä½³Loss':<15} {'æ—¶é—´':<10}")
    print("-" * 80)
    
    for epoch in range(config.num_epochs):
        epoch_start_time = time.time()
        model.train()
        model.likelihood.train()
        
        # æ‰¹é‡è®­ç»ƒï¼ˆå¦‚æœæ•°æ®é‡å¤§ï¼‰
        epoch_loss = 0
        n_batches = 0
        
        if len(X_train) > config.batch_size:
            # éšæœºæ‰“ä¹±
            indices = torch.randperm(len(X_train))
            n_batches_total = (len(X_train) + config.batch_size - 1) // config.batch_size
            
            for i in range(0, len(X_train), config.batch_size):
                batch_indices = indices[i:i+config.batch_size]
                X_batch = X_train[batch_indices]
                y_batch = y_train[batch_indices]
                
                optimizer.zero_grad()
                output = model.gp(X_batch)  # ç›´æ¥ä½¿ç”¨GPå¯¹è±¡
                loss = -mll(output, y_batch)
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                n_batches += 1
        else:
            optimizer.zero_grad()
            output = model.gp(X_train)  # ç›´æ¥ä½¿ç”¨GPå¯¹è±¡
            loss = -mll(output, y_train)
            loss.backward()
            optimizer.step()
            
            epoch_loss = loss.item()
            n_batches = 1
        
        avg_loss = epoch_loss / n_batches
        train_losses.append(avg_loss)
        epoch_time = time.time() - epoch_start_time
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_model_state = model.state_dict().copy()
            best_epoch = epoch + 1
        
        # æ¯10ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochæ‰“å°ä¸€æ¬¡
        if (epoch + 1) % 10 == 0 or (epoch + 1) == config.num_epochs:
            status = "â­" if avg_loss == best_loss else " "
            print(f"{epoch+1:<8} {avg_loss:<15.4f} {best_loss:<15.4f} {epoch_time:<10.2f}s {status}")
    
    training_time = time.time() - training_start_time
    print("-" * 80)
    print(f"è®­ç»ƒå®Œæˆï¼")
    print(f"  - æ€»è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’ ({training_time/60:.2f} åˆ†é’Ÿ)")
    print(f"  - æœ€ä½³Loss: {best_loss:.4f} (Epoch {best_epoch})")
    print(f"  - æœ€ç»ˆLoss: {avg_loss:.4f}")
    print(f"  - å¹³å‡æ¯epochæ—¶é—´: {training_time/config.num_epochs:.2f} ç§’")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nå·²åŠ è½½æœ€ä½³æ¨¡å‹ (Epoch {best_epoch}, Loss={best_loss:.4f})")
    
    # è¯„ä¼°
    print_section_header("æ¨¡å‹è¯„ä¼°")
    evaluation_start_time = time.time()
    
    model.eval()
    model.likelihood.eval()
    
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        # åˆ†æ‰¹é¢„æµ‹ï¼ˆå¦‚æœæµ‹è¯•æ•°æ®é‡å¤§ï¼‰
        pred_mean_list = []
        pred_std_list = []
        
        batch_size = 1000
        for i in range(0, len(X_test), batch_size):
            X_batch = X_test[i:i+batch_size]
            output = model.gp(X_batch)  # ç›´æ¥ä½¿ç”¨GPå¯¹è±¡
            pred_dist = model.likelihood(output)
            
            pred_mean_list.append(pred_dist.mean.cpu().numpy())
            pred_std_list.append(pred_dist.stddev.cpu().numpy())
        
        y_pred_mean = np.concatenate(pred_mean_list)
        y_pred_std = np.concatenate(pred_std_list)
    
    evaluation_time = time.time() - evaluation_start_time
    print(f"é¢„æµ‹å®Œæˆ (è€—æ—¶: {evaluation_time:.2f} ç§’)")
    
    # è®¡ç®—æŒ‡æ ‡
    print("\nè®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    reg_metrics = compute_regression_metrics(y_test_np, y_pred_mean)
    prob_metrics = compute_probabilistic_metrics(y_test_np, y_pred_mean, y_pred_std)
    
    all_metrics = {**reg_metrics, **prob_metrics}
    
    # æ·»åŠ è®­ç»ƒä¿¡æ¯åˆ°ç»“æœ
    all_metrics["experiment_info"] = {
        "experiment_time": experiment_time,
        "random_seed": 42,
        "device": str(device),
        "training_time_seconds": training_time,
        "evaluation_time_seconds": evaluation_time,
        "best_epoch": best_epoch,
        "best_loss": float(best_loss),
        "final_loss": float(avg_loss),
        "model_config": {
            "kernel_space": config.kernel_space,
            "kernel_time": config.kernel_time,
            "num_inducing": config.num_inducing,
            "lr": config.lr,
            "num_epochs": config.num_epochs,
            "batch_size": config.batch_size
        },
        "data_info": {
            "train_points": len(X_train),
            "test_points": len(X_test),
            "n_space_inducing": n_space,
            "n_time_inducing": n_time,
            "total_inducing_points": len(inducing_points)
        }
    }
    
    print("\n" + "=" * 80)
    print("  è¯„ä¼°ç»“æœ")
    print("=" * 80)
    
    # å›å½’æŒ‡æ ‡
    print("\nã€å›å½’æŒ‡æ ‡ã€‘")
    print(f"  {'æŒ‡æ ‡':<30} {'å€¼':<15} {'è¯´æ˜':<30}")
    print("-" * 75)
    print(f"  {'RMSE (Root Mean Squared Error)':<30} {reg_metrics['rmse']:<15.4f} {'è¶Šå°è¶Šå¥½ï¼Œå•ä½: Kelvin'}")
    print(f"  {'MAE (Mean Absolute Error)':<30} {reg_metrics['mae']:<15.4f} {'è¶Šå°è¶Šå¥½ï¼Œå•ä½: Kelvin'}")
    print(f"  {'RÂ² (Coefficient of Determination)':<30} {reg_metrics['r2']:<15.4f} {'è¶Šå¤§è¶Šå¥½ï¼ŒèŒƒå›´: (-âˆ, 1]'}")
    print(f"  {'MAPE (Mean Absolute Percentage Error)':<30} {reg_metrics['mape']:<15.4f} {'è¶Šå°è¶Šå¥½ï¼Œå•ä½: %'}")
    
    # æ¦‚ç‡æŒ‡æ ‡
    print("\nã€æ¦‚ç‡é¢„æµ‹æŒ‡æ ‡ã€‘")
    print(f"  {'æŒ‡æ ‡':<30} {'å€¼':<15} {'è¯´æ˜':<30}")
    print("-" * 75)
    print(f"  {'CRPS (Continuous Ranked Probability Score)':<30} {prob_metrics['crps']:<15.4f} {'è¶Šå°è¶Šå¥½ï¼Œå•ä½: Kelvin'}")
    print(f"  {'Coverage (90% Prediction Interval)':<30} {prob_metrics['coverage_90']:<15.4f} {'ç›®æ ‡: 0.90'}")
    print(f"  {'Interval Width (90%)':<30} {prob_metrics['interval_width_90']:<15.4f} {'è¶Šå°è¶Šå¥½ï¼Œå•ä½: Kelvin'}")
    print(f"  {'Calibration Error':<30} {prob_metrics['calibration_error']:<15.4f} {'è¶Šå°è¶Šå¥½ï¼Œè¡¡é‡æ ¡å‡†åº¦'}")
    
    # é¢„æµ‹ç»Ÿè®¡
    print("\nã€é¢„æµ‹ç»Ÿè®¡ã€‘")
    print(f"  é¢„æµ‹å‡å€¼:")
    print(f"    - èŒƒå›´: [{y_pred_mean.min():.2f}, {y_pred_mean.max():.2f}] K")
    print(f"    - å‡å€¼: {y_pred_mean.mean():.2f} K")
    print(f"    - æ ‡å‡†å·®: {y_pred_mean.std():.2f} K")
    
    print(f"\n  çœŸå®å€¼:")
    print(f"    - èŒƒå›´: [{y_test_np.min():.2f}, {y_test_np.max():.2f}] K")
    print(f"    - å‡å€¼: {y_test_np.mean():.2f} K")
    print(f"    - æ ‡å‡†å·®: {y_test_np.std():.2f} K")
    
    print(f"\n  é¢„æµ‹ä¸ç¡®å®šæ€§ (æ ‡å‡†å·®):")
    print(f"    - èŒƒå›´: [{y_pred_std.min():.2f}, {y_pred_std.max():.2f}] K")
    print(f"    - å‡å€¼: {y_pred_std.mean():.2f} K")
    print(f"    - ä¸­ä½æ•°: {np.median(y_pred_std):.2f} K")
    
    # è¯¯å·®åˆ†æ
    errors = y_test_np - y_pred_mean
    print(f"\nã€è¯¯å·®åˆ†æã€‘")
    print(f"  æ®‹å·® (çœŸå®å€¼ - é¢„æµ‹å€¼):")
    print(f"    - å‡å€¼: {errors.mean():.2f} K (æ¥è¿‘0è¡¨ç¤ºæ— å)")
    print(f"    - æ ‡å‡†å·®: {errors.std():.2f} K")
    print(f"    - èŒƒå›´: [{errors.min():.2f}, {errors.max():.2f}] K")
    print(f"    - ä¸­ä½æ•°: {np.median(errors):.2f} K")
    
    # è¦†ç›–ç‡åˆ†æ
    coverage = prob_metrics['coverage_90']
    target_coverage = 0.90
    coverage_error = abs(coverage - target_coverage)
    print(f"\nã€ä¸ç¡®å®šæ€§æ ¡å‡†ã€‘")
    print(f"  90%é¢„æµ‹åŒºé—´è¦†ç›–ç‡: {coverage:.4f} (ç›®æ ‡: {target_coverage})")
    if coverage_error < 0.05:
        print(f"  âœ… æ ¡å‡†è‰¯å¥½ (è¯¯å·® < 5%)")
    elif coverage_error < 0.10:
        print(f"  âš ï¸  æ ¡å‡†å°šå¯ (è¯¯å·® < 10%)")
    else:
        print(f"  âŒ æ ¡å‡†è¾ƒå·® (è¯¯å·® >= 10%)")
    
    # ä¿å­˜ç»“æœ
    print_section_header("ä¿å­˜ç»“æœ")
    results_path = OUTPUT_DIR / "results" / "gp_results.json"
    with open(results_path, "w") as f:
        json.dump(all_metrics, f, indent=2, ensure_ascii=False)
    print(f"âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜: {results_path}")
    
    # ä¿å­˜æ¨¡å‹
    model_path = OUTPUT_DIR / "models" / "gp_model.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config,
        'inducing_points': inducing_points.cpu(),
        'experiment_info': all_metrics["experiment_info"]
    }, model_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
    print(f"  - æ¨¡å‹å¤§å°: {model_path.stat().st_size / 1024 / 1024:.2f} MB")
    
    # ä¿å­˜è®­ç»ƒæŸå¤±æ›²çº¿
    loss_curve_path = OUTPUT_DIR / "results" / "gp_training_losses.json"
    with open(loss_curve_path, "w") as f:
        json.dump({
            "epochs": list(range(1, len(train_losses) + 1)),
            "losses": train_losses,
            "best_epoch": best_epoch,
            "best_loss": float(best_loss)
        }, f, indent=2)
    print(f"âœ… è®­ç»ƒæŸå¤±æ›²çº¿å·²ä¿å­˜: {loss_curve_path}")
    
    # å¯è§†åŒ–
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    scatter_path = OUTPUT_DIR / "figures" / "gp_scatter.png"
    residuals_path = OUTPUT_DIR / "figures" / "gp_residuals.png"
    
    plot_prediction_scatter(y_test_np, y_pred_mean, save_path=str(scatter_path))
    print(f"âœ… é¢„æµ‹æ•£ç‚¹å›¾å·²ä¿å­˜: {scatter_path}")
    
    plot_residuals(y_test_np, y_pred_mean, save_path=str(residuals_path))
    print(f"âœ… æ®‹å·®å›¾å·²ä¿å­˜: {residuals_path}")
    
    # æ€»ç»“
    total_time = time.time() - start_time
    print_section_header("å®éªŒå®Œæˆ")
    print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")
    print(f"  - æ•°æ®åŠ è½½å’Œé¢„å¤„ç†: {training_start_time - start_time:.2f} ç§’")
    print(f"  - æ¨¡å‹è®­ç»ƒ: {training_time:.2f} ç§’")
    print(f"  - æ¨¡å‹è¯„ä¼°: {evaluation_time:.2f} ç§’")
    print(f"  - ç»“æœä¿å­˜å’Œå¯è§†åŒ–: {total_time - evaluation_time - training_time - (training_start_time - start_time):.2f} ç§’")
    
    print(f"\nä¸»è¦æŒ‡æ ‡æ€»ç»“:")
    print(f"  - RÂ²: {reg_metrics['r2']:.4f}")
    print(f"  - RMSE: {reg_metrics['rmse']:.4f} K")
    print(f"  - CRPS: {prob_metrics['crps']:.4f} K")
    print(f"  - è¦†ç›–ç‡(90%): {prob_metrics['coverage_90']:.4f}")
    
    print(f"\næ‰€æœ‰ç»“æœæ–‡ä»¶:")
    print(f"  ğŸ“„ {results_path}")
    print(f"  ğŸ“„ {loss_curve_path}")
    print(f"  ğŸ’¾ {model_path}")
    print(f"  ğŸ“Š {scatter_path}")
    print(f"  ğŸ“Š {residuals_path}")


if __name__ == "__main__":
    main()

