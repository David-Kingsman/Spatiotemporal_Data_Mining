"""
è®­ç»ƒæ¦‚ç‡U-Netæ¨¡å‹ï¼ˆProbabilistic U-Netï¼‰

æœ¬è„šæœ¬å®ç°äº†åŸºäºU-Netæ¶æ„çš„æ¦‚ç‡æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºMODISåœ°è¡¨æ¸©åº¦æ•°æ®çš„å›¾åƒçº§æ’å€¼å’Œé¢„æµ‹ã€‚

ä¸»è¦ç‰¹ç‚¹ï¼š
1. æ¦‚ç‡è¾“å‡ºï¼šå¯¹æ¯ä¸ªåƒç´ è¾“å‡ºå‡å€¼å’Œæ–¹å·®ï¼ˆlog_varï¼‰ï¼Œæä¾›ä¸ç¡®å®šæ€§ä¼°è®¡
2. U-Netæ¶æ„ï¼šç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œé€‚åˆå›¾åƒinpaintingä»»åŠ¡
3. æ‰¹é‡å½’ä¸€åŒ–ï¼šæé«˜è®­ç»ƒç¨³å®šæ€§
4. Dropoutæ­£åˆ™åŒ–ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
5. è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼šåŸºäºé«˜æ–¯å‡è®¾çš„æ¦‚ç‡æŸå¤±å‡½æ•°

æ•°æ®æ ¼å¼ï¼š
- è¾“å…¥ï¼š3ç»´å¼ é‡ (H, W, T) = (100, 200, 31)
  - H: çº¬åº¦ç»´åº¦ï¼ˆ35Â°-40Â°Nï¼‰
  - W: ç»åº¦ç»´åº¦ï¼ˆ-115Â°--105Â°Wï¼‰
  - T: æ—¶é—´ç»´åº¦ï¼ˆ31å¤©ï¼‰
- è¾“å‡ºï¼šæ¸©åº¦å€¼ï¼ˆå•ä½ï¼šKelvinï¼‰
- ç¼ºå¤±å€¼ï¼šç”¨0è¡¨ç¤º

è¯„ä¼°æŒ‡æ ‡ï¼š
- å›å½’æŒ‡æ ‡ï¼šRMSE, MAE, RÂ², MAPE
- æ¦‚ç‡æŒ‡æ ‡ï¼šCRPS, 90%é¢„æµ‹åŒºé—´è¦†ç›–ç‡, æ ¡å‡†è¯¯å·®

ä½œè€…ï¼šlstinterpå›¢é˜Ÿ
åˆ›å»ºæ—¶é—´ï¼š2024å¹´
"""
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from tqdm import tqdm
import sys
import os
from pathlib import Path
import json
import time
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from lstinterp.data import load_modis_tensor, MODISDataset
from lstinterp.models import ProbUNet, UNetConfig, gaussian_nll_loss
from lstinterp.metrics import compute_regression_metrics, compute_probabilistic_metrics
from lstinterp.viz import plot_mean_map, plot_std_map, plot_error_map
from lstinterp.utils import set_seed

# åˆ›å»ºè¾“å‡ºç›®å½•
OUTPUT_DIR = Path("output")
OUTPUT_DIR.mkdir(exist_ok=True)
(OUTPUT_DIR / "results").mkdir(exist_ok=True)
(OUTPUT_DIR / "figures").mkdir(exist_ok=True)
(OUTPUT_DIR / "models").mkdir(exist_ok=True)


def print_section_header(title, width=80):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * width)
    print(f"  {title}")
    print("=" * width)


def main():
    """ä¸»å‡½æ•°ï¼šè®­ç»ƒå’Œè¯„ä¼°U-Netæ¨¡å‹"""
    start_time = time.time()
    experiment_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    print_section_header("å®éªŒé…ç½®", width=80)
    print(f"å®éªŒæ—¶é—´: {experiment_time}")
    print(f"éšæœºç§å­: 42")
    
    set_seed(42)
    
    # æ£€æŸ¥ä¾èµ–åº“
    print("\nä¾èµ–åº“æ£€æŸ¥:")
    try:
        import torch
        print(f"  âœ… PyTorch: {torch.__version__}")
    except ImportError:
        print("  âŒ PyTorchæœªå®‰è£…")
        return
    
    try:
        import numpy as np
        print(f"  âœ… NumPy: {np.__version__}")
    except ImportError:
        print("  âŒ NumPyæœªå®‰è£…")
        return
    
    # è®¾ç½®è®¾å¤‡ï¼ˆåœ¨å¯¼å…¥torchä¹‹åï¼‰
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"è®¡ç®—è®¾å¤‡: {device}")
    if device.type == "cuda":
        print(f"  - GPUåç§°: {torch.cuda.get_device_name(0)}")
        print(f"  - GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    
    # åŠ è½½æ•°æ®
    print_section_header("æ•°æ®åŠ è½½")
    data_path = "modis_aug_data/MODIS_Aug.mat"
    print(f"æ•°æ®è·¯å¾„: {data_path}")
    
    print("\nåŠ è½½è®­ç»ƒæ•°æ®...")
    train_tensor = load_modis_tensor(data_path, "training_tensor")
    H, W, T = train_tensor.shape
    print(f"è®­ç»ƒæ•°æ®ç»´åº¦: {H} Ã— {W} Ã— {T}")
    
    print("\nåŠ è½½æµ‹è¯•æ•°æ®...")
    test_tensor = load_modis_tensor(data_path, "test_tensor")
    print(f"æµ‹è¯•æ•°æ®ç»´åº¦: {H} Ã— {W} Ã— {T}")
    
    # åˆ›å»ºæ•°æ®é›†
    print_section_header("æ•°æ®é¢„å¤„ç†")
    print("è½¬æ¢ä¸ºå›¾åƒæ•°æ®æ ¼å¼ (T, 1, H, W) â†’ (mean, log_var)")
    
    print("\nåˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼ˆå›¾åƒæ¨¡å¼ï¼‰...")
    train_dataset = MODISDataset(train_tensor, mode="image")
    print(f"  - è®­ç»ƒå›¾åƒæ•°é‡: {len(train_dataset)} å¼  (æ¯å¤©1å¼ )")
    print(f"  - å›¾åƒå°ºå¯¸: {H} Ã— {W} åƒç´ ")
    
    # è·å–è®­ç»ƒé›†çš„å½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼ˆç”¨äºæµ‹è¯•æ—¶ä¿æŒä¸€è‡´ï¼‰
    train_mean = train_dataset.mean_val
    train_std = train_dataset.std_val
    print(f"\næ•°æ®å½’ä¸€åŒ–ç»Ÿè®¡ (Z-score):")
    print(f"  - å‡å€¼: {train_mean:.2f} K")
    print(f"  - æ ‡å‡†å·®: {train_std:.2f} K")
    print(f"  - å½’ä¸€åŒ–èŒƒå›´: çº¦ [{train_mean - 3*train_std:.2f}, {train_mean + 3*train_std:.2f}] K")
    
    # æ•°æ®åŠ è½½å™¨ï¼ˆæ”¹è¿›é…ç½®ï¼‰
    print_section_header("æ¨¡å‹é…ç½®")
    config = UNetConfig(
        batch_size=4,          # æ‰¹å¤§å°ï¼ˆæ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼‰
        num_epochs=50,         # è®­ç»ƒè½®æ•°
        lr=5e-4,               # å­¦ä¹ ç‡
        dropout=0.2,           # Dropoutæ¯”ç‡ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        init_log_var=-1.0      # åˆå§‹log_var=-1ï¼Œå¯¹åº”æ ‡å‡†å·®â‰ˆ0.37ï¼ˆå½’ä¸€åŒ–ååˆç†ï¼‰
    )
    
    print("æ¨¡å‹è¶…å‚æ•°:")
    print(f"  - æ‰¹å¤§å°: {config.batch_size}")
    print(f"  - è®­ç»ƒè½®æ•°: {config.num_epochs}")
    print(f"  - å­¦ä¹ ç‡: {config.lr}")
    print(f"  - Dropout: {config.dropout}")
    print(f"  - åˆå§‹log_var: {config.init_log_var}")
    print(f"  - è¾“å…¥é€šé“æ•°: {config.in_channels} (æ¸©åº¦å›¾ + mask)")
    print(f"  - åŸºç¡€é€šé“æ•°: {config.base_channels}")
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=0,  # Windowså…¼å®¹
        pin_memory=True if device.type == 'cuda' else False
    )
    
    # åˆ›å»ºéªŒè¯é›†ï¼ˆä½¿ç”¨è®­ç»ƒæ•°æ®çš„ä¸€éƒ¨åˆ†ï¼‰
    print("\nåˆ›å»ºè®­ç»ƒ/éªŒè¯åˆ’åˆ†...")
    train_size = len(train_dataset)
    val_size = max(1, int(train_size * 0.1))  # 10%ä½œä¸ºéªŒè¯é›†
    indices = np.random.RandomState(42).permutation(train_size)
    train_indices = indices[val_size:]
    val_indices = indices[:val_size]
    
    print(f"  - è®­ç»ƒé›†: {len(train_indices)} å¼ å›¾åƒ ({len(train_indices)/train_size*100:.1f}%)")
    print(f"  - éªŒè¯é›†: {len(val_indices)} å¼ å›¾åƒ ({len(val_indices)/train_size*100:.1f}%)")
    
    train_subset = Subset(train_dataset, train_indices)
    val_subset = Subset(train_dataset, val_indices)
    
    train_loader = DataLoader(train_subset, batch_size=config.batch_size, shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=config.batch_size, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºæ¨¡å‹...")
    model = ProbUNet(config).to(device)
    
    # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  - æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
    print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # æ¨¡å‹ç»“æ„è¯´æ˜
    print("\næ¨¡å‹ç»“æ„:")
    print("  - æ¶æ„: U-Net (Encoder-Decoder)")
    print("  - ç¼–ç å™¨: å·ç§¯å±‚ + æœ€å¤§æ± åŒ–")
    print("  - è§£ç å™¨: è½¬ç½®å·ç§¯ + ä¸Šé‡‡æ ·")
    print("  - è·³è·ƒè¿æ¥: è¿æ¥ç¼–ç å™¨å’Œè§£ç å™¨çš„å¯¹åº”å±‚")
    print("  - è¾“å‡º: mean (B, 1, H, W) å’Œ log_var (B, 1, H, W)")
    
    # ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼‰
    print("\nä¼˜åŒ–å™¨é…ç½®:")
    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=1e-5)
    print(f"  - ä¼˜åŒ–å™¨: Adam")
    print(f"  - å­¦ä¹ ç‡: {config.lr}")
    print(f"  - æƒé‡è¡°å‡: 1e-5 (L2æ­£åˆ™åŒ–)")
    
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )
    print(f"  - å­¦ä¹ ç‡è°ƒåº¦å™¨: ReduceLROnPlateau")
    print(f"  - é™ä½å› å­: 0.5")
    print(f"  - è€å¿ƒå€¼: 5 epochs")
    
    print(f"\næŸå¤±å‡½æ•°:")
    print(f"  - ç±»å‹: Gaussian Negative Log-Likelihood")
    print(f"  - ä»…åœ¨è§‚æµ‹ç‚¹ä¸Šè®¡ç®—ï¼ˆmask > 0.5ï¼‰")
    
    # è®­ç»ƒï¼ˆå¸¦éªŒè¯é›†ç›‘æ§ï¼‰
    print_section_header("æ¨¡å‹è®­ç»ƒ")
    best_loss = float('inf')
    best_epoch = 1
    patience = 10
    patience_counter = 0
    train_losses = []
    val_losses = []
    training_start_time = time.time()
    
    print(f"å¼€å§‹è®­ç»ƒ ({config.num_epochs} ä¸ªepoch)...")
    print("-" * 100)
    print(f"{'Epoch':<8} {'Train Loss':<15} {'Val Loss':<15} {'Best Val':<15} {'LR':<15} {'æ—¶é—´':<10}")
    print("-" * 100)
    
    for epoch in range(config.num_epochs):
        epoch_start_time = time.time()
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        epoch_loss = 0
        n_batches = 0
        
        for batch_idx, (img, mask, target) in enumerate(train_loader):
            img = img.to(device)
            mask = mask.to(device)
            target = target.to(device)
            
            x = torch.cat([img, mask], dim=1)
            
            optimizer.zero_grad()
            mean, log_var = model(x)
            loss = gaussian_nll_loss(mean, log_var, target, mask)
            
            if torch.isnan(loss) or torch.isinf(loss):
                if batch_idx == 0:  # åªæ‰“å°ç¬¬ä¸€æ¬¡è­¦å‘Š
                    print(f"    âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°æ— æ•ˆloss (NaN/Inf)ï¼Œè·³è¿‡æ­¤batch")
                continue
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
            n_batches += 1
        
        avg_train_loss = epoch_loss / max(n_batches, 1)
        train_losses.append(avg_train_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        n_val_batches = 0
        
        with torch.no_grad():
            for img, mask, target in val_loader:
                img = img.to(device)
                mask = mask.to(device)
                target = target.to(device)
                x = torch.cat([img, mask], dim=1)
                
                mean, log_var = model(x)
                loss = gaussian_nll_loss(mean, log_var, target, mask)
                
                if torch.isfinite(loss):
                    val_loss += loss.item()
                    n_val_batches += 1
        
        avg_val_loss = val_loss / max(n_val_batches, 1) if n_val_batches > 0 else float('inf')
        val_losses.append(avg_val_loss)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        old_lr = optimizer.param_groups[0]['lr']
        scheduler.step(avg_val_loss)
        new_lr = optimizer.param_groups[0]['lr']
        lr_reduced = (new_lr < old_lr)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_loss:
            best_loss = avg_val_loss
            best_epoch = epoch + 1
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
        
        epoch_time = time.time() - epoch_start_time
        current_lr = optimizer.param_groups[0]['lr']
        status = "â­" if avg_val_loss == best_loss else ("ğŸ“‰" if lr_reduced else " ")
        
        # æ¯5ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochæ‰“å°ä¸€æ¬¡
        if (epoch + 1) % 5 == 0 or (epoch + 1) == config.num_epochs or (epoch + 1) == 1:
            print(f"{epoch+1:<8} {avg_train_loss:<15.4f} {avg_val_loss:<15.4f} {best_loss:<15.4f} {current_lr:<15.6f} {epoch_time:<10.2f}s {status}")
        
        # æ—©åœ
        if patience_counter >= patience:
            print(f"\næ—©åœè§¦å‘ï¼ˆpatience={patience}ï¼‰ï¼Œæ¢å¤æœ€ä½³æ¨¡å‹ (Epoch {best_epoch})")
            model.load_state_dict(best_model_state)
            break
    
    training_time = time.time() - training_start_time
    print("-" * 100)
    print(f"è®­ç»ƒå®Œæˆï¼")
    print(f"  - æ€»è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’ ({training_time/60:.2f} åˆ†é’Ÿ)")
    print(f"  - æœ€ä½³éªŒè¯Loss: {best_loss:.4f} (Epoch {best_epoch})")
    print(f"  - æœ€ç»ˆè®­ç»ƒLoss: {avg_train_loss:.4f}")
    print(f"  - æœ€ç»ˆéªŒè¯Loss: {avg_val_loss:.4f}")
    print(f"  - å¹³å‡æ¯epochæ—¶é—´: {training_time/(epoch+1):.2f} ç§’")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if 'best_model_state' in locals():
        model.load_state_dict(best_model_state)
        print(f"\nå·²åŠ è½½æœ€ä½³æ¨¡å‹ (Epoch {best_epoch}, éªŒè¯Loss={best_loss:.4f})")
    
    # è¯„ä¼°
    print_section_header("æ¨¡å‹è¯„ä¼°")
    evaluation_start_time = time.time()
    model.eval()
    
    # åœ¨æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„å½’ä¸€åŒ–å‚æ•°ï¼‰
    test_dataset = MODISDataset(test_tensor, mode="image", norm_mean=train_mean, norm_std=train_std)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
    
    all_preds_mean = []
    all_preds_std = []
    all_targets = []
    all_masks = []
    
    with torch.no_grad():
        for img, mask, target in test_loader:
            img = img.to(device)
            mask = mask.to(device)
            target = target.to(device)
            x = torch.cat([img, mask], dim=1)
            
            mean, log_var = model(x)
            std = torch.exp(0.5 * log_var)
            
            # è½¬ç§»åˆ°CPUå†è½¬numpy
            all_preds_mean.append(mean.cpu().numpy())
            all_preds_std.append(std.cpu().numpy())
            all_targets.append(target.cpu().numpy())
            all_masks.append(mask.cpu().numpy())
    
    # åˆå¹¶ç»“æœ
    pred_mean = np.concatenate(all_preds_mean, axis=0)[:, 0, :, :]  # (T, H, W)
    pred_std = np.concatenate(all_preds_std, axis=0)[:, 0, :, :]
    targets = np.concatenate(all_targets, axis=0)[:, 0, :, :]
    masks = np.concatenate(all_masks, axis=0)[:, 0, :, :]
    
    # åå½’ä¸€åŒ–é¢„æµ‹ç»“æœï¼ˆæ¢å¤åˆ°åŸå§‹å°ºåº¦ï¼‰
    # ä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    mean_val = train_mean
    std_val = train_std
    
    # åªåœ¨æœ‰è§‚æµ‹çš„ç‚¹ä¸Šè¯„ä¼°
    valid_mask = masks > 0.5
    y_true_norm = targets[valid_mask]  # å½’ä¸€åŒ–çš„çœŸå®å€¼
    y_pred_norm = pred_mean[valid_mask]  # å½’ä¸€åŒ–çš„é¢„æµ‹å€¼
    y_std_norm = pred_std[valid_mask]  # å½’ä¸€åŒ–çš„æ ‡å‡†å·®
    
    # åå½’ä¸€åŒ–
    y_true = y_true_norm * std_val + mean_val
    y_pred = y_pred_norm * std_val + mean_val
    y_std = y_std_norm * std_val
    
    evaluation_time = time.time() - evaluation_start_time
    print(f"é¢„æµ‹å®Œæˆ (è€—æ—¶: {evaluation_time:.2f} ç§’)")
    print(f"  - æœ‰æ•ˆé¢„æµ‹ç‚¹æ•°: {len(y_true):,} (ä»…åœ¨è§‚æµ‹ç‚¹ä¸Šè¯„ä¼°)")
    
    # è®¡ç®—æŒ‡æ ‡
    print("\nè®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    reg_metrics = compute_regression_metrics(y_true, y_pred)
    prob_metrics = compute_probabilistic_metrics(y_true, y_pred, y_std)
    
    all_metrics = {**reg_metrics, **prob_metrics}
    
    # æ·»åŠ å®éªŒä¿¡æ¯
    all_metrics["experiment_info"] = {
        "experiment_time": experiment_time,
        "random_seed": 42,
        "device": str(device),
        "training_time_seconds": training_time,
        "evaluation_time_seconds": evaluation_time,
        "best_epoch": best_epoch,
        "best_val_loss": float(best_loss),
        "final_train_loss": float(avg_train_loss),
        "final_val_loss": float(avg_val_loss),
        "model_config": {
            "batch_size": config.batch_size,
            "num_epochs": config.num_epochs,
            "lr": config.lr,
            "dropout": config.dropout,
            "init_log_var": config.init_log_var,
            "in_channels": config.in_channels,
            "base_channels": config.base_channels
        },
        "data_info": {
            "train_images": len(train_indices),
            "val_images": len(val_indices),
            "test_images": T,
            "image_size": f"{H}Ã—{W}",
            "normalization": {
                "mean": float(train_mean),
                "std": float(train_std)
            },
            "valid_test_points": len(y_true)
        }
    }
    
    print("\n" + "=" * 80)
    print("  è¯„ä¼°ç»“æœ")
    print("=" * 80)
    
    # å›å½’æŒ‡æ ‡
    print("\nã€å›å½’æŒ‡æ ‡ã€‘")
    print(f"  {'æŒ‡æ ‡':<30} {'å€¼':<15} {'è¯´æ˜':<30}")
    print("-" * 75)
    print(f"  {'RMSE (Root Mean Squared Error)':<30} {reg_metrics['rmse']:<15.4f} {'è¶Šå°è¶Šå¥½ï¼Œå•ä½: Kelvin'}")
    print(f"  {'MAE (Mean Absolute Error)':<30} {reg_metrics['mae']:<15.4f} {'è¶Šå°è¶Šå¥½ï¼Œå•ä½: Kelvin'}")
    print(f"  {'RÂ² (Coefficient of Determination)':<30} {reg_metrics['r2']:<15.4f} {'è¶Šå¤§è¶Šå¥½ï¼ŒèŒƒå›´: (-âˆ, 1]'}")
    print(f"  {'MAPE (Mean Absolute Percentage Error)':<30} {reg_metrics['mape']:<15.4f} {'è¶Šå°è¶Šå¥½ï¼Œå•ä½: %'}")
    
    # æ¦‚ç‡æŒ‡æ ‡
    print("\nã€æ¦‚ç‡é¢„æµ‹æŒ‡æ ‡ã€‘")
    print(f"  {'æŒ‡æ ‡':<30} {'å€¼':<15} {'è¯´æ˜':<30}")
    print("-" * 75)
    print(f"  {'CRPS (Continuous Ranked Probability Score)':<30} {prob_metrics['crps']:<15.4f} {'è¶Šå°è¶Šå¥½ï¼Œå•ä½: Kelvin'}")
    print(f"  {'Coverage (90% Prediction Interval)':<30} {prob_metrics['coverage_90']:<15.4f} {'ç›®æ ‡: 0.90'}")
    print(f"  {'Interval Width (90%)':<30} {prob_metrics['interval_width_90']:<15.4f} {'è¶Šå°è¶Šå¥½ï¼Œå•ä½: Kelvin'}")
    print(f"  {'Calibration Error':<30} {prob_metrics['calibration_error']:<15.4f} {'è¶Šå°è¶Šå¥½ï¼Œè¡¡é‡æ ¡å‡†åº¦'}")
    
    # é¢„æµ‹ç»Ÿè®¡
    print("\nã€é¢„æµ‹ç»Ÿè®¡ã€‘")
    print(f"  é¢„æµ‹å‡å€¼:")
    print(f"    - èŒƒå›´: [{y_pred.min():.2f}, {y_pred.max():.2f}] K")
    print(f"    - å‡å€¼: {y_pred.mean():.2f} K")
    print(f"    - æ ‡å‡†å·®: {y_pred.std():.2f} K")
    
    print(f"\n  çœŸå®å€¼:")
    print(f"    - èŒƒå›´: [{y_true.min():.2f}, {y_true.max():.2f}] K")
    print(f"    - å‡å€¼: {y_true.mean():.2f} K")
    print(f"    - æ ‡å‡†å·®: {y_true.std():.2f} K")
    
    print(f"\n  é¢„æµ‹ä¸ç¡®å®šæ€§ (æ ‡å‡†å·®):")
    print(f"    - èŒƒå›´: [{y_std.min():.2f}, {y_std.max():.2f}] K")
    print(f"    - å‡å€¼: {y_std.mean():.2f} K")
    print(f"    - ä¸­ä½æ•°: {np.median(y_std):.2f} K")
    
    # è¯¯å·®åˆ†æ
    errors = y_true - y_pred
    print(f"\nã€è¯¯å·®åˆ†æã€‘")
    print(f"  æ®‹å·® (çœŸå®å€¼ - é¢„æµ‹å€¼):")
    print(f"    - å‡å€¼: {errors.mean():.2f} K (æ¥è¿‘0è¡¨ç¤ºæ— å)")
    print(f"    - æ ‡å‡†å·®: {errors.std():.2f} K")
    print(f"    - èŒƒå›´: [{errors.min():.2f}, {errors.max():.2f}] K")
    print(f"    - ä¸­ä½æ•°: {np.median(errors):.2f} K")
    
    # è¦†ç›–ç‡åˆ†æ
    coverage = prob_metrics['coverage_90']
    target_coverage = 0.90
    coverage_error = abs(coverage - target_coverage)
    print(f"\nã€ä¸ç¡®å®šæ€§æ ¡å‡†ã€‘")
    print(f"  90%é¢„æµ‹åŒºé—´è¦†ç›–ç‡: {coverage:.4f} (ç›®æ ‡: {target_coverage})")
    if coverage_error < 0.05:
        print(f"  âœ… æ ¡å‡†è‰¯å¥½ (è¯¯å·® < 5%)")
    elif coverage_error < 0.10:
        print(f"  âš ï¸  æ ¡å‡†å°šå¯ (è¯¯å·® < 10%)")
    else:
        print(f"  âŒ æ ¡å‡†è¾ƒå·® (è¯¯å·® >= 10%)")
    
    # ä¿å­˜ç»“æœ
    print_section_header("ä¿å­˜ç»“æœ")
    results_path = OUTPUT_DIR / "results" / "unet_results.json"
    with open(results_path, "w") as f:
        json.dump(all_metrics, f, indent=2, ensure_ascii=False)
    print(f"âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜: {results_path}")
    
    # ä¿å­˜æ¨¡å‹
    model_path = OUTPUT_DIR / "models" / "unet_model.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config,
        'experiment_info': all_metrics["experiment_info"]
    }, model_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
    print(f"  - æ¨¡å‹å¤§å°: {model_path.stat().st_size / 1024 / 1024:.2f} MB")
    
    # ä¿å­˜è®­ç»ƒæŸå¤±æ›²çº¿
    loss_curve_path = OUTPUT_DIR / "results" / "unet_training_losses.json"
    with open(loss_curve_path, "w") as f:
        json.dump({
            "epochs": list(range(1, len(train_losses) + 1)),
            "train_losses": train_losses,
            "val_losses": val_losses,
            "best_epoch": best_epoch,
            "best_val_loss": float(best_loss)
        }, f, indent=2)
    print(f"âœ… è®­ç»ƒæŸå¤±æ›²çº¿å·²ä¿å­˜: {loss_curve_path}")
    
    # åå½’ä¸€åŒ–å¯è§†åŒ–ç”¨çš„æ•°æ®ï¼ˆæ¢å¤åˆ°åŸå§‹å°ºåº¦ï¼‰
    pred_mean_denorm = pred_mean * std_val + mean_val
    pred_std_denorm = pred_std * std_val
    targets_denorm = targets * std_val + mean_val
    
    # å¯è§†åŒ–ï¼ˆç¬¬15å¤©ï¼‰
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    day_idx = 14
    mean_path = OUTPUT_DIR / "figures" / "unet_mean_day15.png"
    std_path = OUTPUT_DIR / "figures" / "unet_std_day15.png"
    error_path = OUTPUT_DIR / "figures" / "unet_error_day15.png"
    
    plot_mean_map(
        pred_mean_denorm, day_idx=day_idx,
        title="U-Net Mean Prediction - Day 15",
        save_path=str(mean_path)
    )
    print(f"âœ… é¢„æµ‹å‡å€¼å›¾å·²ä¿å­˜: {mean_path}")
    
    plot_std_map(
        pred_std_denorm, day_idx=day_idx,
        title="U-Net Prediction Uncertainty - Day 15",
        save_path=str(std_path)
    )
    print(f"âœ… é¢„æµ‹ä¸ç¡®å®šæ€§å›¾å·²ä¿å­˜: {std_path}")
    
    plot_error_map(
        targets_denorm, pred_mean_denorm, day_idx=day_idx,
        title="U-Net Prediction Error - Day 15",
        save_path=str(error_path)
    )
    print(f"âœ… é¢„æµ‹è¯¯å·®å›¾å·²ä¿å­˜: {error_path}")
    
    # æ€»ç»“
    total_time = time.time() - start_time
    print_section_header("å®éªŒå®Œæˆ")
    print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")
    print(f"  - æ•°æ®åŠ è½½å’Œé¢„å¤„ç†: {training_start_time - start_time:.2f} ç§’")
    print(f"  - æ¨¡å‹è®­ç»ƒ: {training_time:.2f} ç§’")
    print(f"  - æ¨¡å‹è¯„ä¼°: {evaluation_time:.2f} ç§’")
    
    print(f"\nä¸»è¦æŒ‡æ ‡æ€»ç»“:")
    print(f"  - RÂ²: {reg_metrics['r2']:.4f}")
    print(f"  - RMSE: {reg_metrics['rmse']:.4f} K")
    print(f"  - CRPS: {prob_metrics['crps']:.4f} K")
    print(f"  - è¦†ç›–ç‡(90%): {prob_metrics['coverage_90']:.4f}")
    
    print(f"\næ‰€æœ‰ç»“æœæ–‡ä»¶:")
    print(f"  ğŸ“„ {results_path}")
    print(f"  ğŸ“„ {loss_curve_path}")
    print(f"  ğŸ’¾ {model_path}")
    print(f"  ğŸ“Š {mean_path}")
    print(f"  ğŸ“Š {std_path}")
    print(f"  ğŸ“Š {error_path}")


if __name__ == "__main__":
    main()

